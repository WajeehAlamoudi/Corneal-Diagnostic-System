{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T17:01:14.665063Z",
     "start_time": "2025-12-04T17:01:14.625752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1200 * 910 the standers, pls edit below for test image dim\n",
    "configuration = {\n",
    "\n",
    "    \"resize\": (224, 224),\n",
    "\n",
    "    \"crop_positions\": {\n",
    "        \n",
    "        \"Q1\": {\"x1\": 772, \"y1\": 91, \"x2\": (772+360), \"y2\": (91+360), \"center\": (967,275), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q2\": {\"x1\": 410, \"y1\": 90, \"x2\": (410+360), \"y2\": (90+360), \"center\": (592, 275), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q3\": {\"x1\": 410, \"y1\": 467, \"x2\": (410+360), \"y2\": (467+360), \"center\": (592,652), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q4\": {\"x1\": 771, \"y1\": 467, \"x2\": (771+360), \"y2\": (467+360), \"center\": (967,652), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"text_panel\": {\"x1\": 10, \"y1\": 200, \"x2\": (10+315), \"y2\": (200+625)}\n",
    "    }\n",
    "}\n"
   ],
   "id": "d1a19ba5c7ccbc00",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T19:08:54.677481Z",
     "start_time": "2025-12-04T19:08:54.614518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import colorsys\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from colorthief import ColorThief\n",
    "\n",
    "\n",
    "class VisionModule:\n",
    "    \n",
    "    def __init__(self, feature_extraction_model, configuration):\n",
    "        self.handcraft_features = None\n",
    "        self.feature_extraction_model = feature_extraction_model\n",
    "        self.configuration = configuration\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.label_map = {  \n",
    "            \"normal\": 0,\n",
    "            \"Keratoconus\": 1\n",
    "        }\n",
    "        self.test_label_map = {  \n",
    "            0: \"normal\",\n",
    "            1: \"Keratoconus\"\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def crop(self, img, img_name=\"UNKNOWN\", save_dir=None):\n",
    "\n",
    "        crops = {}\n",
    "        crop_cfg = self.configuration[\"crop_positions\"]\n",
    "\n",
    "        for key, cfg in crop_cfg.items():\n",
    "            \n",
    "            # --- 1. Crop the region ---\n",
    "            x1, y1, x2, y2 = cfg[\"x1\"], cfg[\"y1\"], cfg[\"x2\"], cfg[\"y2\"]\n",
    "            crop_img = img[y1:y2, x1:x2].copy()\n",
    "            \n",
    "            # --- 2. Apply circular mask ONLY if mask_radius_factor exists ---\n",
    "            if cfg.get(\"apply_circular_mask\", False) is True:\n",
    "                radius = cfg.get(\"radius\", None)\n",
    "                center = cfg.get(\"center\", None)\n",
    "                center = (center[0] - x1, center[1] - y1)\n",
    "                crop_img = self._apply_circular_mask(\n",
    "                    crop_img,\n",
    "                    radius,\n",
    "                    center\n",
    "                )\n",
    "\n",
    "            # --- 3. Optionally save cropped image ---\n",
    "            if save_dir is not None and key != \"text_panel\":\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                save_path = os.path.join(save_dir, f\"{img_name}_{key}.png\")\n",
    "                cv2.imwrite(save_path, cv2.cvtColor(crop_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            # --- 4. Add to output dictionary ---\n",
    "            crops[key] = crop_img\n",
    "\n",
    "        return crops\n",
    "\n",
    "\n",
    "    def _apply_circular_mask(self, img, radius, center):\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        if center is not None:\n",
    "            center = (int(center[0]), int(center[1]))\n",
    "        else:\n",
    "            center = (int(w // 2), int(h // 2))\n",
    "            \n",
    "        if radius is not None:\n",
    "            radius = int(radius)\n",
    "        else:\n",
    "            radius = int(min(h, w) * 0.9 / 2)\n",
    "    \n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    \n",
    "        # make white circle on a center\n",
    "        cv2.circle(mask, center, radius, (255, 255, 255), -1)\n",
    "    \n",
    "        mask = mask.astype(float) / 255.0\n",
    "        \n",
    "        fill_color = [0.485*255, 0.456*255, 0.406*255]\n",
    "        fill_color = np.array(fill_color, dtype=np.float32)\n",
    "        \n",
    "        fill_img = np.ones_like(img, dtype=np.float32) * fill_color\n",
    "    \n",
    "        result = img.astype(float) * mask[..., None] + fill_img * (1 - mask[..., None])\n",
    "    \n",
    "        return result.astype(np.uint8)\n",
    "        \n",
    "    \n",
    "    def preprocess_for_cnn(self, cropped_img):\n",
    "        # resize\n",
    "        size = self.configuration[\"resize\"]\n",
    "        img = cv2.resize(cropped_img, size)\n",
    "    \n",
    "        # convert to float32 0â€“1\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "        # ImageNet norm (change if you use custom model)\n",
    "        mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "        std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    \n",
    "        img = (img - mean) / std\n",
    "    \n",
    "        # HWC â†’ CHW\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "    \n",
    "        return img\n",
    "    \n",
    "      \n",
    "    def run_image_preprocessing(self, image_path):\n",
    "\n",
    "        class_dir = os.path.dirname(image_path)  # e.g. dataset/normal/images\n",
    "        base_dir = os.path.dirname(class_dir)    # e.g. dataset/normal\n",
    "    \n",
    "        # save_text_dir   = os.path.join(base_dir, \"text_panels\")\n",
    "        save_crops_dir  = os.path.join(base_dir, \"crops\")\n",
    "    \n",
    "        # os.makedirs(save_text_dir, exist_ok=True)\n",
    "        os.makedirs(save_crops_dir, exist_ok=True)\n",
    "    \n",
    "        img_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    \n",
    "        bgr = cv2.imread(image_path)\n",
    "        if bgr is None:\n",
    "            raise FileNotFoundError(f\"Could not read: {image_path}\")\n",
    "    \n",
    "        img = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        crops = self.crop(img, img_name=img_name, save_dir=save_crops_dir)\n",
    "    \n",
    "        # Save all crops for debugging / training\n",
    "        for key, crop_rgb in crops.items():\n",
    "            if key == \"text_panel\":\n",
    "                continue\n",
    "            else:\n",
    "                out_path = os.path.join(save_crops_dir, f\"{img_name}_{key}.png\")\n",
    "                cv2.imwrite(out_path, cv2.cvtColor(crop_rgb, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "        # # Save text panel for OCR\n",
    "        # text_crop_rgb = crops[\"text_panel\"]\n",
    "        # text_path = os.path.join(save_text_dir, f\"{img_name}_textpanel.png\")\n",
    "        # \n",
    "        # cv2.imwrite(text_path, cv2.cvtColor(text_crop_rgb, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "        return save_crops_dir\n",
    "    \n",
    "\n",
    "    def run_vision_preprocessing(self, folder_path):\n",
    "\n",
    "        exts = (\".jpg\", \".jpeg\", \".png\", \".bmp\")\n",
    "        all_images = [\n",
    "            os.path.join(folder_path, f)\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.lower().endswith(exts)\n",
    "        ]\n",
    "    \n",
    "        print(f\"[INFO] Found {len(all_images)} images in: {folder_path}\")\n",
    "    \n",
    "       # parent class folder (normal or Keratoconus)\n",
    "        class_dir = os.path.dirname(folder_path)\n",
    "    \n",
    "        # resolved main output dirs\n",
    "        crops_dir = os.path.join(class_dir, \"crops\")\n",
    "        # deep_dir = os.path.join(class_dir, \"deep_features\")\n",
    "        # hand_dir = os.path.join(class_dir, \"handcraft_features\")\n",
    "        # text_panel_dir = os.path.join(class_dir, \"text_panels\")\n",
    "        \n",
    "        processed_list = []\n",
    "\n",
    "        for img_path in all_images:\n",
    "            try:\n",
    "                print(f\"[INFO] Processing: {img_path}\")\n",
    "                self.run_image_preprocessing(img_path)\n",
    "                \n",
    "                processed_list.append(os.path.basename(img_path))\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed on {img_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "        print(\"[INFO] Completed folder processing.\")\n",
    "        \n",
    "        return {\n",
    "            \"crops\": crops_dir,\n",
    "            \"processed_images\": processed_list\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def extract_deep_features(self, tensor, save_dir=None, img_name=None, quadrant=None):\n",
    "\n",
    "        t = torch.from_numpy(tensor).unsqueeze(0).float().to(self.device)\n",
    "        # shape (1,3,224,224) for resnet 50\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            deep_features = self.feature_extraction_model(t).cpu().numpy().flatten()\n",
    "            # === OPTIONAL SAVE ===\n",
    "        if save_dir is not None:\n",
    "    \n",
    "            # Ensure folder exists\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "            # Build filename\n",
    "            # Example: \"image001_Q1.npy\"\n",
    "            if img_name is None:\n",
    "                img_name = \"unknown\"\n",
    "    \n",
    "            if quadrant is None:\n",
    "                quadrant = \"QX\"\n",
    "    \n",
    "            filename = f\"{img_name}_{quadrant}.npy\"\n",
    "            output_path = os.path.join(save_dir, filename)\n",
    "    \n",
    "            # Save features\n",
    "            np.save(output_path, deep_features)\n",
    "        return deep_features\n",
    "    \n",
    "\n",
    "    def handcrafted_features(self, cropped_img_pth, save_dir=None, img_name=None, quadrant=None):\n",
    "            \n",
    "        colors_platte = ColorThief(cropped_img_pth)\n",
    "        dominant_colors = colors_platte.get_palette(color_count=2)\n",
    "        # print(dominant_colors)\n",
    "        \n",
    "        ignored_colors = [\n",
    "            (0, 0, 0),\n",
    "            (255, 255, 255),\n",
    "            (int(0.485*255), int(0.456*255), int(0.406*255))  # fill mask base color\n",
    "        ]\n",
    "        tol = 15\n",
    "        cleaned_colors = []\n",
    "        \n",
    "        for R, G, B in dominant_colors:\n",
    "            bad = False\n",
    "            for ir, ig, ib in ignored_colors:\n",
    "                if (abs(R - ir) <= tol and\n",
    "                    abs(G - ig) <= tol and\n",
    "                    abs(B - ib) <= tol):\n",
    "                    bad = True\n",
    "                    break\n",
    "        \n",
    "            if not bad:\n",
    "                cleaned_colors.append((R, G, B))\n",
    "        \n",
    "        if len(cleaned_colors) == 0:\n",
    "            cleaned_colors = [(0,0,0)]\n",
    "        \n",
    "        R, G, B = cleaned_colors[0]\n",
    "        r, g, b = R/255.0, G/255.0, B/255.0\n",
    "        \n",
    "        h, s, v = colorsys.rgb_to_hsv(r, g, b)\n",
    "        \n",
    "        dom_h = h\n",
    "        dom_s = s\n",
    "        dom_v = v\n",
    "        \n",
    "        # ---- FIXED BGR â†’ RGB for cv2 ----\n",
    "        cropped_img = cv2.imread(cropped_img_pth)\n",
    "        cropped_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        img = cropped_img.astype(np.float32) / 255.0\n",
    "        hsv = cv2.cvtColor(cropped_img, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "    \n",
    "        # Create mask of \"valid\" pixels matrix/grid\n",
    "        valid_mask = np.ones((img.shape[0], img.shape[1]), dtype=bool)\n",
    "    \n",
    "        for ir, ig, ib in ignored_colors:\n",
    "            if (ir, ig, ib) == ignored_colors[2]:\n",
    "                # This is the FILL MASK â†’ use tolerance\n",
    "                invalid = (\n",
    "                    (np.abs(cropped_img[:,:,0] - ir) < tol) &\n",
    "                    (np.abs(cropped_img[:,:,1] - ig) < tol) &\n",
    "                    (np.abs(cropped_img[:,:,2] - ib) < tol)\n",
    "                )\n",
    "            else:\n",
    "                # Black & white â†’ exact match\n",
    "                invalid = (\n",
    "                    (cropped_img[:,:,0] == ir) &\n",
    "                    (cropped_img[:,:,1] == ig) &\n",
    "                    (cropped_img[:,:,2] == ib)\n",
    "                )\n",
    "        \n",
    "            valid_mask[invalid] = False\n",
    "    \n",
    "        # Valid pixels only\n",
    "        valid_hsv = hsv[valid_mask]\n",
    "    \n",
    "        # Intensity Features (based on V channel)\n",
    "        V = valid_hsv[:,2] / 255.0\n",
    "    \n",
    "        avg_intensity = float(V.mean())\n",
    "        std_intensity = float(V.std())\n",
    "    \n",
    "        # Center vs Periphery intensities, the center sharpness\n",
    "        h_im, w_im = cropped_img.shape[:2]\n",
    "        cy, cx = h_im//2, w_im//2\n",
    "        R = min(cx, cy)\n",
    "    \n",
    "        r_center = int(R * 0.3)\n",
    "        r_mid = int(R * 0.60)\n",
    "    \n",
    "        Y, X = np.ogrid[:h_im, :w_im]\n",
    "        dist = np.sqrt((X - cx)**2 + (Y - cy)**2)\n",
    "    \n",
    "        center_mask = dist < r_center\n",
    "        periphery_mask = (dist > r_mid) & (dist < R)\n",
    "    \n",
    "        center_vals = img[center_mask][:, :].mean() if center_mask.any() else 0.0\n",
    "        periphery_vals = img[periphery_mask][:, :].mean() if periphery_mask.any() else 1e-6\n",
    "        \n",
    "        center_intensity = float(center_vals)\n",
    "        periphery_intensity = float(periphery_vals)\n",
    "        center_periphery_ratio = float(center_intensity / periphery_intensity)\n",
    "    \n",
    "        # Quadrant splits and asymmetry ratios\n",
    "        top = img[:h_im//2,:,:].mean()\n",
    "        bottom = img[h_im//2:,:,:].mean()\n",
    "        left = img[:, :w_im//2, :].mean()\n",
    "        right = img[:, w_im//2:, :].mean()\n",
    "    \n",
    "        inferior_superior_ratio = float(bottom / (top + 1e-6))\n",
    "        left_right_ratio = float(left / (right + 1e-6))\n",
    "    \n",
    "        diag1 = img[:h_im//2, :w_im//2, :].mean() - img[h_im//2:, w_im//2:, :].mean()\n",
    "        diag2 = img[:h_im//2, w_im//2:, :].mean() - img[h_im//2:, :w_im//2, :].mean()\n",
    "    \n",
    "        diag1_difference = float(diag1)\n",
    "        diag2_difference = float(diag2)\n",
    "    \n",
    "        radial_symmetry = float(\n",
    "            abs(top - bottom) +\n",
    "            abs(left - right) +\n",
    "            abs(diag1) +\n",
    "            abs(diag2)\n",
    "        )\n",
    "    \n",
    "        # ----- PACK RESULTS -----\n",
    "        features = {\n",
    "            \"dom_h\": dom_h,\n",
    "            \"dom_s\": dom_s,\n",
    "            \"dom_v\": dom_v,\n",
    "    \n",
    "            \"avg_intensity\": avg_intensity,\n",
    "            \"std_intensity\": std_intensity,\n",
    "    \n",
    "            \"center_intensity\": center_intensity,\n",
    "            \"periphery_intensity\": periphery_intensity,\n",
    "            \"center_periphery_ratio\": center_periphery_ratio,\n",
    "    \n",
    "            \"inferior_superior_ratio\": inferior_superior_ratio,\n",
    "            \"left_right_ratio\": left_right_ratio,\n",
    "    \n",
    "            \"diag1_difference\": diag1_difference,\n",
    "            \"diag2_difference\": diag2_difference,\n",
    "    \n",
    "            \"radial_symmetry\": radial_symmetry\n",
    "        }\n",
    "        self.handcraft_features = features\n",
    "    \n",
    "        # ============== OPTIONAL SAVE ==============\n",
    "        if save_dir is not None:\n",
    "    \n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "            if img_name is None:\n",
    "                img_name = \"unknown\"\n",
    "    \n",
    "            if quadrant is None:\n",
    "                quadrant = \"QX\"\n",
    "    \n",
    "            filename = f\"{img_name}_{quadrant}.json\"\n",
    "            output_path = os.path.join(save_dir, filename)\n",
    "    \n",
    "            # save handcrafted features as JSON\n",
    "            with open(output_path, \"w\") as f:\n",
    "                json.dump(features, f, indent=4)\n",
    "    \n",
    "        return features\n",
    "    "
   ],
   "id": "199bcba3ab306a9d",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:52:09.268996Z",
     "start_time": "2025-12-04T13:52:07.771999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "feature_extractor = nn.Sequential(*list(backbone.children())[:-1])\n",
    "feature_extractor = feature_extractor.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "dcde4263e8fbbef8",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:52:10.107330Z",
     "start_time": "2025-12-04T13:52:10.101329Z"
    }
   },
   "cell_type": "code",
   "source": "vision_obj = VisionModule(feature_extraction_model=feature_extractor, configuration=configuration)",
   "id": "4ad37c86f6e22e5f",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:27:57.186203Z",
     "start_time": "2025-12-04T13:27:55.399456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_paths = [r\"dataset/Keratoconus/images\", r\"dataset/normal/images\"] \n",
    "\n",
    "outputs={}\n",
    "\n",
    "for dataset_path in dataset_paths:\n",
    "    class_name = os.path.basename(os.path.dirname(dataset_path))\n",
    "    outputs[class_name] = vision_obj.run_vision_preprocessing(dataset_path)\n"
   ],
   "id": "f464e43d167dc7ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 10 images in: dataset/Keratoconus/images\n",
      "[INFO] Processing: dataset/Keratoconus/images\\53.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\55.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\56.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\57.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\59.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\61.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\62.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\65.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\66.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\67.jpg\n",
      "[INFO] Completed folder processing.\n",
      "[INFO] Found 10 images in: dataset/normal/images\n",
      "[INFO] Processing: dataset/normal/images\\207.jpg\n",
      "[INFO] Processing: dataset/normal/images\\208.jpg\n",
      "[INFO] Processing: dataset/normal/images\\209.jpg\n",
      "[INFO] Processing: dataset/normal/images\\210.jpg\n",
      "[INFO] Processing: dataset/normal/images\\211.jpg\n",
      "[INFO] Processing: dataset/normal/images\\212.jpg\n",
      "[INFO] Processing: dataset/normal/images\\213.jpg\n",
      "[INFO] Processing: dataset/normal/images\\214.jpg\n",
      "[INFO] Processing: dataset/normal/images\\215.jpg\n",
      "[INFO] Processing: dataset/normal/images\\216.jpg\n",
      "[INFO] Completed folder processing.\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:52:49.271342Z",
     "start_time": "2025-12-04T13:52:27.755169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for class_name, info in outputs.items():\n",
    "\n",
    "        print(f\"\\n[INFO] Processing class: {class_name}\")\n",
    "\n",
    "        crops_dir = info[\"crops\"]                   \n",
    "        processed_images = info[\"processed_images\"] \n",
    "        class_base = os.path.dirname(crops_dir)     \n",
    "\n",
    "        # Create output dirs\n",
    "        deep_dir = os.path.join(class_base, \"deep_features\")\n",
    "        hand_dir = os.path.join(class_base, \"handcraft_features\")\n",
    "\n",
    "        os.makedirs(deep_dir, exist_ok=True)\n",
    "        os.makedirs(hand_dir, exist_ok=True)\n",
    "\n",
    "        for img_name in processed_images:\n",
    "\n",
    "            img_id = os.path.splitext(img_name)[0]\n",
    "\n",
    "            print(f\"[INFO] Extracting features for: {img_id}\")\n",
    "\n",
    "            for q in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
    "\n",
    "                crop_path = os.path.join(crops_dir, f\"{img_id}_{q}.png\")\n",
    "\n",
    "                if not os.path.exists(crop_path):\n",
    "                    print(f\"[WARN] Missing crop: {crop_path}\")\n",
    "                    continue\n",
    "\n",
    "                # -----------------------------\n",
    "                # Load crop\n",
    "                # -----------------------------\n",
    "                crop = cv2.imread(crop_path)\n",
    "                crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # -----------------------------\n",
    "                # Deep features (CNN)\n",
    "                # -----------------------------\n",
    "                cnn_ready = vision_obj.preprocess_for_cnn(crop)\n",
    "                deep_vec = vision_obj.extract_deep_features(\n",
    "                    tensor=cnn_ready,\n",
    "                    save_dir=deep_dir,\n",
    "                    img_name=img_id,\n",
    "                    quadrant=q\n",
    "                )\n",
    "\n",
    "                # -----------------------------\n",
    "                # Handcrafted features\n",
    "                # -----------------------------\n",
    "                vision_obj.handcrafted_features(\n",
    "                    cropped_img_pth=crop_path,\n",
    "                    save_dir=hand_dir,\n",
    "                    img_name=img_id,\n",
    "                    quadrant=q\n",
    "                )\n",
    "\n",
    "        print(f\"[INFO] Completed extracting features for class {class_name}\")"
   ],
   "id": "5d27df5fe62c1912",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing class: Keratoconus\n",
      "[INFO] Extracting features for: 53\n",
      "[INFO] Extracting features for: 55\n",
      "[INFO] Extracting features for: 56\n",
      "[INFO] Extracting features for: 57\n",
      "[INFO] Extracting features for: 59\n",
      "[INFO] Extracting features for: 61\n",
      "[INFO] Extracting features for: 62\n",
      "[INFO] Extracting features for: 65\n",
      "[INFO] Extracting features for: 66\n",
      "[INFO] Extracting features for: 67\n",
      "[INFO] Completed extracting features for class Keratoconus\n",
      "\n",
      "[INFO] Processing class: normal\n",
      "[INFO] Extracting features for: 207\n",
      "[INFO] Extracting features for: 208\n",
      "[INFO] Extracting features for: 209\n",
      "[INFO] Extracting features for: 210\n",
      "[INFO] Extracting features for: 211\n",
      "[INFO] Extracting features for: 212\n",
      "[INFO] Extracting features for: 213\n",
      "[INFO] Extracting features for: 214\n",
      "[INFO] Extracting features for: 215\n",
      "[INFO] Extracting features for: 216\n",
      "[INFO] Completed extracting features for class normal\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<- The end of vision module pre preparations ->\n",
    "\n",
    "<- The start of text module pre preparations -> \n"
   ],
   "id": "fd441a3deee36a40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T20:25:19.497065Z",
     "start_time": "2025-12-04T20:25:19.481737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import ViTModel, ViTImageProcessor\n",
    "\n",
    "\n",
    "class TextModule:\n",
    "\n",
    "    def __init__(self, model_name=\"google/vit-base-patch16-224-in21k\", device=None):\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        # Load transformer\n",
    "        self.processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "        self.model = ViTModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    def extract_transformer_features(self, img_path: str, save_dir=None, img_name=None, quadrant=None):\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "        # Preprocess\n",
    "        inputs = self.processor(images=img, return_tensors=\"pt\").to(self.device)\n",
    "    \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            out = self.model(**inputs)\n",
    "    \n",
    "        # CLS token embedding\n",
    "        vec = out.pooler_output.squeeze(0).cpu().numpy()\n",
    "    \n",
    "        # --------------------------------------------------------\n",
    "        # Conditional saving (ONLY IF save_dir is provided)\n",
    "        # --------------------------------------------------------\n",
    "        if save_dir is not None:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "            # build file name\n",
    "            if img_name is None:\n",
    "                # default: use last component of img_path\n",
    "                img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    \n",
    "            if quadrant is not None:\n",
    "                file_name = f\"{img_name}_{quadrant}.npy\"\n",
    "            else:\n",
    "                file_name = f\"{img_name}.npy\"\n",
    "    \n",
    "            save_path = os.path.join(save_dir, file_name)\n",
    "    \n",
    "            # save the vector\n",
    "            np.save(save_path, vec)\n",
    "    \n",
    "        # return vector always\n",
    "        return vec\n"
   ],
   "id": "6623b018fb22c400",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T18:44:36.552787Z",
     "start_time": "2025-12-04T18:44:35.574481Z"
    }
   },
   "cell_type": "code",
   "source": "text_obj = TextModule()",
   "id": "24ec2fe2097b2143",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T18:44:59.415749Z",
     "start_time": "2025-12-04T18:44:39.147475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for class_name, info in outputs.items():\n",
    "\n",
    "    print(f\"\\n[INFO] Processing class: {class_name}\")\n",
    "\n",
    "    crops_dir = info[\"crops\"]\n",
    "    processed_images = info[\"processed_images\"]\n",
    "    class_base = os.path.dirname(crops_dir)\n",
    "\n",
    "\n",
    "    transformer_dir = os.path.join(class_base, \"transformer_features\")\n",
    "    os.makedirs(transformer_dir, exist_ok=True)\n",
    "\n",
    "    for img_name in processed_images:\n",
    "\n",
    "        img_id = os.path.splitext(img_name)[0]\n",
    "        print(f\"[INFO] Extracting transformer features for: {img_id}\")\n",
    "\n",
    "        for q in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
    "            \n",
    "            crop_path = os.path.join(crops_dir, f\"{img_id}_{q}.png\")\n",
    "\n",
    "            if not os.path.exists(crop_path):\n",
    "                print(f\"[WARN] Missing crop: {crop_path}\")\n",
    "                continue\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # Transformer features (ViT)\n",
    "            # ---------------------------------------------------\n",
    "            _ = text_obj.extract_transformer_features(\n",
    "                img_path=crop_path,\n",
    "                save_dir=transformer_dir,\n",
    "                img_name=img_id,\n",
    "                quadrant=q\n",
    "            )\n",
    "\n",
    "    print(f\"[INFO] Completed transformer features for class {class_name}\")"
   ],
   "id": "be447301e919a77c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing class: Keratoconus\n",
      "[INFO] Extracting transformer features for: 53\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\53_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\53_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\53_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\53_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 55\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\55_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\55_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\55_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\55_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 56\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\56_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\56_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\56_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\56_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 57\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\57_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\57_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\57_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\57_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 59\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\59_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\59_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\59_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\59_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 61\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\61_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\61_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\61_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\61_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 62\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\62_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\62_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\62_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\62_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 65\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\65_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\65_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\65_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\65_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 66\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\66_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\66_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\66_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\66_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 67\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\67_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\67_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\67_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/Keratoconus\\transformer_features\\67_Q4.npy | shape=(768,)\n",
      "[INFO] Completed transformer features for class Keratoconus\n",
      "\n",
      "[INFO] Processing class: normal\n",
      "[INFO] Extracting transformer features for: 207\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\207_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\207_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\207_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\207_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 208\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\208_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\208_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\208_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\208_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 209\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\209_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\209_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\209_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\209_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 210\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\210_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\210_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\210_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\210_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 211\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\211_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\211_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\211_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\211_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 212\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\212_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\212_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\212_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\212_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 213\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\213_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\213_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\213_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\213_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 214\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\214_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\214_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\214_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\214_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 215\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\215_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\215_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\215_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\215_Q4.npy | shape=(768,)\n",
      "[INFO] Extracting transformer features for: 216\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\216_Q1.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\216_Q2.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\216_Q3.npy | shape=(768,)\n",
      "[Transformer] Saved vector to dataset/normal\\transformer_features\\216_Q4.npy | shape=(768,)\n",
      "[INFO] Completed transformer features for class normal\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T19:31:30.000103Z",
     "start_time": "2025-12-04T19:31:29.930391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "class XGBoostTrainer:\n",
    "    def __init__(self, dataset_root=\"dataset\"):\n",
    "        self.dataset_root = dataset_root\n",
    "        self.classes = [\"normal\", \"Keratoconus\"]\n",
    "        \n",
    "        \n",
    "    def load_quadrant_features(self, class_dir, img_id, quadrant):\n",
    "    \n",
    "        # ---------------------------\n",
    "        # Paths\n",
    "        # ---------------------------\n",
    "        deep_pth = os.path.join(class_dir, \"deep_features\", f\"{img_id}_{quadrant}.npy\")\n",
    "        hand_pth = os.path.join(class_dir, \"handcraft_features\", f\"{img_id}_{quadrant}.json\")\n",
    "        trans_pth = os.path.join(class_dir, \"transformer_features\", f\"{img_id}_{quadrant}.npy\")\n",
    "    \n",
    "        # Must exist\n",
    "        if not (os.path.exists(deep_pth) and os.path.exists(hand_pth) and os.path.exists(trans_pth)):\n",
    "            return None\n",
    "    \n",
    "        # ---------------------------\n",
    "        # Deep features\n",
    "        # ---------------------------\n",
    "        deep_vec = np.load(deep_pth).astype(np.float32)\n",
    "    \n",
    "        # ---------------------------\n",
    "        # Handcrafted features (JSON â†’ full concatenation)\n",
    "        # ---------------------------\n",
    "        with open(hand_pth, \"r\") as f:\n",
    "            hand_dict = json.load(f)\n",
    "    \n",
    "        # flatten ALL handcrafted values into correct order\n",
    "        hand_vec = np.array([v for _, v in hand_dict.items()], dtype=np.float32)\n",
    "    \n",
    "        # ---------------------------\n",
    "        # Transformer features\n",
    "        # ---------------------------\n",
    "        trans_vec = np.load(trans_pth).astype(np.float32)\n",
    "    \n",
    "        # ---------------------------\n",
    "        # CONCAT ALL THREE MODALITIES\n",
    "        # ---------------------------\n",
    "        fused = np.concatenate([deep_vec, hand_vec, trans_vec], axis=0)\n",
    "    \n",
    "        return fused\n",
    "\n",
    "\n",
    "    def build_dataset(self):\n",
    "    \n",
    "        X = []\n",
    "        y = []\n",
    "    \n",
    "        for label_idx, cls_name in enumerate(self.classes):\n",
    "    \n",
    "            class_dir = os.path.join(self.dataset_root, cls_name)\n",
    "            crops_dir = os.path.join(class_dir, \"crops\")\n",
    "    \n",
    "            # get image ids from crops folder\n",
    "            img_files = [f for f in os.listdir(crops_dir) if \"_Q1\" in f]\n",
    "            img_ids = [f.split(\"_\")[0] for f in img_files]\n",
    "    \n",
    "            for img_id in img_ids:\n",
    "    \n",
    "                quadrant_vectors = []\n",
    "    \n",
    "                for q in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
    "                    vec = self.load_quadrant_features(class_dir, img_id, q)\n",
    "                    if vec is not None:\n",
    "                        quadrant_vectors.append(vec)\n",
    "    \n",
    "                if len(quadrant_vectors) == 0:\n",
    "                    print(f\"[WARN] Skipping {img_id}, missing features.\")\n",
    "                    continue\n",
    "    \n",
    "                # â­ OPTION 1: AVERAGE QUADRANTS (recommended)\n",
    "                fused_vec = np.mean(quadrant_vectors, axis=0)\n",
    "    \n",
    "                # â­ OPTION 2: CONCATENATE ALL QUADRANTS (HUGE VECTOR)\n",
    "                # fused_vec = np.concatenate(quadrant_vectors, axis=0)\n",
    "    \n",
    "                X.append(fused_vec)\n",
    "                y.append(label_idx)\n",
    "    \n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        y = np.array(y, dtype=np.int32)\n",
    "    \n",
    "        print(\"[INFO] Dataset shape:\", X.shape, \"Labels:\", y.shape)\n",
    "        return X, y\n",
    "    \n",
    "\n",
    "    def train(self, save_path=\"xgboost_model.pkl\", n_estimators=200, depth=5, lr=0.05):\n",
    "\n",
    "        X, y = self.build_dataset()\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=depth,\n",
    "            learning_rate=lr,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\"\n",
    "        )\n",
    "\n",
    "        print(\"[INFO] Training XGBoost...\")\n",
    "        model.fit(X, y)\n",
    "\n",
    "        joblib.dump(model, save_path)\n",
    "        print(f\"[INFO] Model saved to: {save_path}\")\n",
    "\n",
    "        return model\n"
   ],
   "id": "f09d7cf87d9c8362",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T19:31:30.585099Z",
     "start_time": "2025-12-04T19:31:30.216436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = XGBoostTrainer(dataset_root=\"dataset\")\n",
    "xgb_model = trainer.train(r\"saved_models/kc_classifier.pkl\")"
   ],
   "id": "9a0e3f96b63a2708",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset shape: (20, 2829) Labels: (20,)\n",
      "[INFO] Training XGBoost...\n",
      "[INFO] Model saved to: saved_models/kc_classifier.pkl\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T20:10:13.974630Z",
     "start_time": "2025-12-04T20:10:10.673719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Union, List, Dict\n",
    "import base64\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.genai.errors import APIError\n",
    "\n",
    "class GEMINIAGENT:\n",
    "\n",
    "    def __init__(self, api_key: str, system_message: str = None, model: str = None):\n",
    "        \"\"\"Initializes the Gemini client and chat session.\"\"\"\n",
    "        print(f\"Initializing GEMINIAGENT chat for model {model}.\")\n",
    "\n",
    "        self.model = model\n",
    "        self.system_message = system_message\n",
    "        self.history = []  # types.Content objects will be managed by the chat session.\n",
    "        self.last_usage_metadata = None  # For token usage retrieval\n",
    "        self.chat_session = None\n",
    "\n",
    "        try:\n",
    "            # 1. Initialization: Client setup uses the API key\n",
    "            self.client = genai.Client(api_key=api_key)\n",
    "\n",
    "            # Optional: Verify API key by listing models (similar to OpenAI logic)\n",
    "            # The genai client does not have a simple 'models.list()'. A simple call\n",
    "            # with a quick timeout can serve as a proxy for key validation.\n",
    "            # For simplicity, we skip this complex step but rely on the subsequent\n",
    "            # chat creation to validate the key and connection.\n",
    "\n",
    "            # 2. Chat Session Creation: This is where conversation history is managed\n",
    "            # The system instruction is part of the initial config.\n",
    "            config = None\n",
    "            if self.system_message:\n",
    "                config = types.GenerateContentConfig(\n",
    "                    system_instruction=self.system_message\n",
    "                )\n",
    "\n",
    "            self.chat_session = self.client.chats.create(\n",
    "                model=self.model,\n",
    "                config=config,\n",
    "            )\n",
    "\n",
    "            print(f\"GEMINIAGENT initialized successfully for model {model}.\")\n",
    "\n",
    "        except APIError as e:\n",
    "            print(\"Failed to initialize Gemini client\", e)\n",
    "            raise ValueError(f\"Invalid or unauthorized API key or connection error: {e}\") from e\n",
    "        except Exception as e:\n",
    "            print(\"Unexpected error during GEMINIAGENT initialization\", e)\n",
    "            raise ValueError(f\"Unexpected error while initializing client: {e}\") from e\n",
    "\n",
    "    def ask(self, user_input: Union[str, Dict[str, Union[str, List[str]]]], response_type: str = None) -> str:\n",
    "        \"\"\"Sends a message to the model and returns the text response.\"\"\"\n",
    "\n",
    "        if not user_input:\n",
    "            raise ValueError(\"ask agent failed: Please enter a valid user_input.\")\n",
    "\n",
    "        # 1. Build the 'contents' list (list of Parts)\n",
    "        parts: List[types.Part] = []\n",
    "\n",
    "        # ðŸ”¹ Handle text + image multimodal input\n",
    "        if isinstance(user_input, dict) and \"prompt\" in user_input:\n",
    "            # expected format: {\"prompt\": \"...\", \"images\": [\"img1.png\", \"img2.jpg\"]}\n",
    "\n",
    "            # Add text part\n",
    "            parts.append(types.Part(text=user_input[\"prompt\"]))\n",
    "\n",
    "            # Add image parts (using local file paths)\n",
    "            for img_path in user_input.get(\"images\", []):\n",
    "                path = Path(img_path)\n",
    "                if not path.exists():\n",
    "                    raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "\n",
    "                # Determine MIME type\n",
    "                mime_map = {\".jpg\": \"image/jpeg\", \".jpeg\": \"image/jpeg\", \".png\": \"image/png\"}\n",
    "                mime = mime_map.get(path.suffix.lower(), \"application/octet-stream\")\n",
    "\n",
    "                with open(path, \"rb\") as f:\n",
    "                    image_bytes = f.read()\n",
    "\n",
    "                # Use Part.from_bytes for local image files\n",
    "                parts.append(types.Part.from_bytes(\n",
    "                    data=image_bytes,\n",
    "                    mime_type=mime\n",
    "                ))\n",
    "\n",
    "        else:\n",
    "            # Handle text-only input\n",
    "            parts.append(types.Part(text=user_input))\n",
    "\n",
    "        # 2. Configure the generation request\n",
    "        config = types.GenerateContentConfig()\n",
    "\n",
    "        # 3. Handle JSON/Structured Output request\n",
    "        if response_type == \"json_object\":\n",
    "            # Gemini uses response_mime_type for JSON output\n",
    "            # A schema can also be provided, but for simple JSON_OBJECT, this is the minimal change\n",
    "            config.response_mime_type = \"application/json\"\n",
    "\n",
    "        # 'json_schema' is a more complex case involving response_schema and is omitted for brevity,\n",
    "        # but would map to types.GenerateContentConfig(response_schema=...)\n",
    "\n",
    "        # 4. Send the message\n",
    "        try:\n",
    "            # Check if the config has been modified for JSON output\n",
    "            is_config_modified = (\n",
    "                    hasattr(config, \"response_mime_type\") and\n",
    "                    config.response_mime_type == \"application/json\"\n",
    "            )\n",
    "\n",
    "            # Build keyword arguments for the send_message call\n",
    "            request_kwargs = {}\n",
    "            if is_config_modified:\n",
    "                request_kwargs[\"config\"] = config\n",
    "\n",
    "            # Pass the content (parts) and any configuration\n",
    "            response = self.chat_session.send_message(\n",
    "                parts,  # Pass the message content\n",
    "                **request_kwargs  # Pass the config if present\n",
    "            )\n",
    "\n",
    "            reply = response.text.strip()\n",
    "\n",
    "            # Save token usage metadata\n",
    "            self.last_usage_metadata = response.usage_metadata\n",
    "\n",
    "            print(f\"GEMINIAGENT.ask() completed successfully with reply length {len(reply)} chars.\")\n",
    "            return reply\n",
    "\n",
    "        except APIError as e:\n",
    "            time.sleep(35)\n",
    "            # Handle context length exceeded specifically\n",
    "            # Gemini's APIError will contain context limits, but a universal retry is more complex\n",
    "            # than the OpenAI one. The most direct equivalent is to clear and re-raise.\n",
    "            if \"context_length\" in str(e).lower() or \"quota\" in str(e).lower():\n",
    "                # self.clear_conversation_history()\n",
    "                # time.sleep(5)\n",
    "                # # Re-send the *current* user input, which includes the multimodal handling logic above.\n",
    "                # return self.ask(user_input=user_input, response_type=response_type)\n",
    "                raise ValueError(f\"Gemini Context length or quota issue detected: {e}\") from e\n",
    "            else:\n",
    "                raise ValueError(f\"Gemini chat completion failed: {e}\") from e\n",
    "\n",
    "        except Exception as e:\n",
    "            time.sleep(35)\n",
    "            raise ValueError(f\"Unexpected error during chat completion: {e}\") from e\n",
    "\n",
    "    def clear_conversation_history(self):\n",
    "        \"\"\"Clear the conversation history by recreating the chat session.\"\"\"\n",
    "        print(\"Clearing GEMINIAGENT conversation history...\")\n",
    "\n",
    "        old_length = len(self.chat_session.get_history())\n",
    "        print(f\"Previous conversation length: {old_length} message(s).\")\n",
    "\n",
    "        # Reset by creating a new chat session with the original model and config\n",
    "        config = types.GenerateContentConfig(system_instruction=self.system_message) if self.system_message else None\n",
    "        self.chat_session = self.client.chats.create(\n",
    "            model=self.model,\n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "        print(\"Conversation history cleared successfully.\")\n",
    "\n",
    "    def get_token_usage(self):\n",
    "        \"\"\"\n",
    "        Return token usage from the last API call using usage_metadata.\n",
    "        Returns (prompt_tokens, completion_tokens, total_tokens) or None if unavailable.\n",
    "        \"\"\"\n",
    "        print(\"Entering GEMINIAGENT.get_token_usage()\")\n",
    "\n",
    "        if self.last_usage_metadata:\n",
    "            # The GenAI SDK returns tokens under 'usage_metadata'\n",
    "            prompt = self.last_usage_metadata.prompt_token_count or 0\n",
    "            completion = self.last_usage_metadata.candidates_token_count or 0\n",
    "            total = self.last_usage_metadata.total_token_count or 0\n",
    "\n",
    "            print(\n",
    "                f\"Token usage retrieved: prompt={prompt}, completion={completion}, total={total}\"\n",
    "            )\n",
    "            return prompt, completion, total\n",
    "\n",
    "        print(\"No token usage data available in GEMINIAGENT.\")\n",
    "        return None"
   ],
   "id": "eade4cc78f7c8b20",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TEST PIPELINE below",
   "id": "347f2de57ce53fb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T20:25:28.968712Z",
     "start_time": "2025-12-04T20:25:28.961990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "configuration = {\n",
    "\n",
    "    \"resize\": (224, 224),\n",
    "\n",
    "    \"crop_positions\": {\n",
    "        \n",
    "        \"Q1\": {\"x1\": 772, \"y1\": 91, \"x2\": (772+360), \"y2\": (91+360), \"center\": (967,275), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q2\": {\"x1\": 410, \"y1\": 90, \"x2\": (410+360), \"y2\": (90+360), \"center\": (592, 275), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q3\": {\"x1\": 410, \"y1\": 467, \"x2\": (410+360), \"y2\": (467+360), \"center\": (592,652), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q4\": {\"x1\": 771, \"y1\": 467, \"x2\": (771+360), \"y2\": (467+360), \"center\": (967,652), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"text_panel\": {\"x1\": 10, \"y1\": 200, \"x2\": (10+315), \"y2\": (200+625)}\n",
    "    }\n",
    "}"
   ],
   "id": "32a9ff8397c42259",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T20:25:29.751090Z",
     "start_time": "2025-12-04T20:25:29.158827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "feature_extractor = nn.Sequential(*list(backbone.children())[:-1])\n",
    "feature_extractor = feature_extractor.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "1c6ce28ca659413b",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T20:31:08.875404Z",
     "start_time": "2025-12-04T20:31:05.259544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vision_model_obj = VisionModule( feature_extraction_model=feature_extractor, configuration=configuration )\n",
    "text_model_obj = TextModule()\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "\n",
    "def test_architecture_pipeline(img_path):\n",
    "\n",
    "    print(\"\\n==================== TEST PIPELINE ====================\")\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 1. Preprocess â†’ get crops folder\n",
    "    # ---------------------------------------------------\n",
    "    crops_path = vision_model_obj.run_image_preprocessing(img_path)\n",
    "\n",
    "    if crops_path is None or not os.path.exists(crops_path):\n",
    "        print(\"[ERROR] run_image_preprocessing() did not return a valid folder.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 2. Prepare save dirs (TEMP STORAGE for test)\n",
    "    # ---------------------------------------------------\n",
    "    base = os.path.dirname(os.path.dirname(img_path))\n",
    "\n",
    "    deep_dir = os.path.join(base, \"test_deep_features\")\n",
    "    hand_dir = os.path.join(base, \"test_hand_features\")\n",
    "    trans_dir = os.path.join(base, \"test_transformer_features\")\n",
    "\n",
    "    os.makedirs(deep_dir, exist_ok=True)\n",
    "    os.makedirs(hand_dir, exist_ok=True)\n",
    "    os.makedirs(trans_dir, exist_ok=True)\n",
    "\n",
    "    img_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 3. Extract features for each quadrant\n",
    "    # ---------------------------------------------------\n",
    "    quadrant_list = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
    "    quadrant_vectors = []\n",
    "    handcrafted_all_quadrants = {}\n",
    "\n",
    "    for q in quadrant_list:\n",
    "\n",
    "        crop_path = os.path.join(crops_path, f\"{img_id}_{q}.png\")\n",
    "\n",
    "        if not os.path.exists(crop_path):\n",
    "            print(f\"[WARN] Missing crop: {crop_path}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # LOAD CROP\n",
    "        crop = cv2.imread(crop_path)\n",
    "        crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # DEEP FEATURES\n",
    "        # ---------------------------------------------------\n",
    "        cnn_ready = vision_model_obj.preprocess_for_cnn(crop)\n",
    "        deep_vec = vision_model_obj.extract_deep_features(\n",
    "            tensor=cnn_ready,\n",
    "            save_dir=deep_dir,\n",
    "            img_name=img_id,\n",
    "            quadrant=q\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # HANDCRAFT FEATURES\n",
    "        # ---------------------------------------------------\n",
    "        hand_vec = vision_model_obj.handcrafted_features(\n",
    "            cropped_img_pth=crop_path,\n",
    "            save_dir=hand_dir,\n",
    "            img_name=img_id,\n",
    "            quadrant=q\n",
    "        )\n",
    "        handcrafted_all_quadrants[q] = hand_vec\n",
    "\n",
    "        hand_vec = np.array(list(hand_vec.values()), dtype=np.float32)\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # TRANSFORMER FEATURES\n",
    "        # ---------------------------------------------------\n",
    "        trans_vec = text_model_obj.extract_transformer_features(\n",
    "            img_path=crop_path,\n",
    "            save_dir=trans_dir,\n",
    "            img_name=img_id,\n",
    "            quadrant=q\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # CONCATENATE 3 MODALITIES FOR THIS QUADRANT\n",
    "        # ---------------------------------------------------\n",
    "        fused_vec = np.concatenate([deep_vec, hand_vec, trans_vec], axis=0)\n",
    "        quadrant_vectors.append(fused_vec)\n",
    "\n",
    "    if len(quadrant_vectors) == 0:\n",
    "        print(\"[ERROR] No valid quadrant vectors extracted.\")\n",
    "        return None\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 4. FINAL IMAGE FEATURE = AVERAGE OF QUADRANTS\n",
    "    # ---------------------------------------------------\n",
    "    final_feature_vec = np.mean(quadrant_vectors, axis=0)\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 5. LOAD TRAINED XGBOOST MODEL\n",
    "    # ---------------------------------------------------\n",
    "    model_path = r\"saved_models/kc_classifier.pkl\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"[ERROR] XGBoost model file not found:\", model_path)\n",
    "        return None\n",
    "\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 6. PREDICTION\n",
    "    # ---------------------------------------------------\n",
    "    pred_prob = model.predict_proba(final_feature_vec.reshape(1, -1))[0]\n",
    "    pred_class = model.predict(final_feature_vec.reshape(1, -1))[0]\n",
    "\n",
    "    label_map = {0: \"normal\", 1: \"Keratoconus\"}\n",
    "    pred_label = label_map[int(pred_class)]\n",
    "\n",
    "    print(\"\\n==================== RESULT ====================\")\n",
    "    print(f\"Prediction: {pred_label}\")\n",
    "    print(f\"Probability: normal={pred_prob[0]:.4f}, KC={pred_prob[1]:.4f}\")\n",
    "    print(\"=================================================\\n\")\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 7. OUTPUTS FOR LLM REPORT\n",
    "    # ---------------------------------------------------\n",
    "    final_prediction = {\n",
    "        \"image_id\": img_id,\n",
    "        \"predicted_label\": pred_label,\n",
    "        \"probabilities\": {\n",
    "            \"normal\": float(pred_prob[0]),\n",
    "            \"keratoconus\": float(pred_prob[1])\n",
    "        },\n",
    "        \"handcrafted_summary\": handcrafted_all_quadrants\n",
    "    }\n",
    "    \n",
    "    plain_prompt = f\"\"\"\n",
    "        Image ID: {img_id}\n",
    "        \n",
    "        Prediction: {pred_label}\n",
    "        Probability Normal: {final_prediction['probabilities']['normal']:.4f}\n",
    "        Probability Keratoconus: {final_prediction['probabilities']['keratoconus']:.4f}\n",
    "        \n",
    "        Handcrafted features per quadrant:\n",
    "        {handcrafted_all_quadrants}\n",
    "        \n",
    "        Please summarize this result clinically.\n",
    "        \"\"\"\n",
    "    \n",
    "    return final_prediction, plain_prompt\n",
    "    "
   ],
   "id": "d218f0a077b249bc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 58707953-f3d2-4bbb-913c-2d0299f7a6bb)')' thrown while requesting HEAD https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/preprocessor_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T20:38:41.759913Z",
     "start_time": "2025-12-04T20:38:41.758914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def run_with_chat(img_path):\n",
    "\n",
    "    # 1. Run your full vision + XGBoost pipeline\n",
    "    final_prediction, prompt_text = test_architecture_pipeline(img_path)\n",
    "\n",
    "    # 2. Create Gemini agent with basic system message\n",
    "    agent = GEMINIAGENT(\n",
    "        api_key=\"AIzaSyC1r4egWoskczgSYmXkxCQMhJ6szqOw6XM\",\n",
    "        system_message=\n",
    "        \"\"\"\n",
    "        You are a medical assistant and OCULUS - PENTACAM 4 Maps Refractive analysis .\n",
    "        answer the user's question and try to explain the had craft for each Q and its mean.\n",
    "        \"\"\",\n",
    "        model=\"gemini-2.5-flash\"\n",
    "    )\n",
    "\n",
    "    # 3. First automatic summary\n",
    "    report = agent.ask(prompt_text)\n",
    "    display(Markdown(\"## Clinical Report\\n\\n\" + report))\n",
    "\n",
    "\n",
    "    # 4. Start the chat loop\n",
    "    print(\"\\n===== CHAT MODE (type 'exit' to quit) =====\")\n",
    "\n",
    "    # --- minimal change: add a text widget ---\n",
    "    input_box = widgets.Text(\n",
    "        placeholder=\"Type here...\",\n",
    "        description=\"You:\",\n",
    "        layout=widgets.Layout(width=\"500px\")\n",
    "    )\n",
    "    \n",
    "    display(input_box)\n",
    "    \n",
    "    while True:\n",
    "        # wait until user presses Enter in widget\n",
    "        user_input = input_box.value.strip()\n",
    "    \n",
    "        if user_input == \"\":\n",
    "            continue  # no message yet â†’ do not continue loop\n",
    "        input_box.value = \"\"  # clear after user sends\n",
    "    \n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Chat ended.\")\n",
    "            break\n",
    "    \n",
    "        reply = agent.ask(user_input)\n",
    "    \n",
    "        # Display markdown output\n",
    "        display(Markdown(f\"### Assistant\\n\\n{reply}\"))\n",
    "\n"
   ],
   "id": "83ee79112e03d6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "run_with_chat(\"dataset/test/images/2.jpg\")",
   "id": "8e6bfdaeedbb6e88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a5bc449dbcff0def"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
