{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T19:37:13.125264Z",
     "start_time": "2025-12-03T19:37:13.073750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1200 * 910 the standers, pls edit below for test image dim\n",
    "configuration = {\n",
    "\n",
    "    \"resize\": (224, 224),\n",
    "\n",
    "    \"crop_positions\": {\n",
    "        \n",
    "        \"Q1\": {\"x1\": 772, \"y1\": 91, \"x2\": (772+360), \"y2\": (91+360), \"center\": (967,275), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q2\": {\"x1\": 410, \"y1\": 90, \"x2\": (410+360), \"y2\": (90+360), \"center\": (592, 275), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q3\": {\"x1\": 410, \"y1\": 467, \"x2\": (410+360), \"y2\": (467+360), \"center\": (592,652), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q4\": {\"x1\": 771, \"y1\": 467, \"x2\": (771+360), \"y2\": (467+360), \"center\": (967,652), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"text_panel\": {\"x1\": 10, \"y1\": 200, \"x2\": (10+315), \"y2\": (200+625-72)}\n",
    "    }\n",
    "}"
   ],
   "id": "d1a19ba5c7ccbc00",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T19:37:13.150896Z",
     "start_time": "2025-12-03T19:37:13.129584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "class XGBoostVisionClassifier:\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_estimators=400,\n",
    "                 max_depth=6,\n",
    "                 learning_rate=0.05,\n",
    "                 subsample=0.9,\n",
    "                 colsample_bytree=0.9,\n",
    "                 random_state=42):\n",
    "\n",
    "        self.model = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    # ---------------------------------------\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    def save(self, path):\n",
    "        joblib.dump(self.model, path)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    def load(self, path):\n",
    "        self.model = joblib.load(path)\n",
    "        return self\n"
   ],
   "id": "ea59977f722e076",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:09:10.476598Z",
     "start_time": "2025-12-03T20:09:10.442188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import colorsys\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from colorthief import ColorThief\n",
    "\n",
    "\n",
    "class VisionModule:\n",
    "    \n",
    "    def __init__(self, feature_extraction_model, classifier, configuration):\n",
    "        self.handcraft_features = None\n",
    "        self.feature_extraction_model = feature_extraction_model\n",
    "        self.classifier = classifier\n",
    "        self.configuration = configuration\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.label_map = {  \n",
    "            \"normal\": 0,\n",
    "            \"Keratoconus\": 1\n",
    "        }\n",
    "        self.test_label_map = {  \n",
    "            0: \"normal\",\n",
    "            1: \"Keratoconus\"\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def crop(self, img, img_name=\"UNKNOWN\", save_dir=None):\n",
    "\n",
    "        crops = {}\n",
    "        crop_cfg = self.configuration[\"crop_positions\"]\n",
    "\n",
    "        for key, cfg in crop_cfg.items():\n",
    "            \n",
    "            # --- 1. Crop the region ---\n",
    "            x1, y1, x2, y2 = cfg[\"x1\"], cfg[\"y1\"], cfg[\"x2\"], cfg[\"y2\"]\n",
    "            crop_img = img[y1:y2, x1:x2].copy()\n",
    "            \n",
    "            # --- 2. Apply circular mask ONLY if mask_radius_factor exists ---\n",
    "            if cfg.get(\"apply_circular_mask\", False) is True:\n",
    "                radius = cfg.get(\"radius\", None)\n",
    "                center = cfg.get(\"center\", None)\n",
    "                center = (center[0] - x1, center[1] - y1)\n",
    "                crop_img = self._apply_circular_mask(\n",
    "                    crop_img,\n",
    "                    radius,\n",
    "                    center\n",
    "                )\n",
    "\n",
    "            # --- 3. Optionally save cropped image ---\n",
    "            if save_dir is not None:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                save_path = os.path.join(save_dir, f\"{img_name}_{key}.png\")\n",
    "                cv2.imwrite(save_path, cv2.cvtColor(crop_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            # --- 4. Add to output dictionary ---\n",
    "            crops[key] = crop_img\n",
    "\n",
    "        return crops\n",
    "\n",
    "\n",
    "    def _apply_circular_mask(self, img, radius, center):\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        if center is not None:\n",
    "            center = (int(center[0]), int(center[1]))\n",
    "        else:\n",
    "            center = (int(w // 2), int(h // 2))\n",
    "            \n",
    "        if radius is not None:\n",
    "            radius = int(radius)\n",
    "        else:\n",
    "            radius = int(min(h, w) * 0.9 / 2)\n",
    "    \n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    \n",
    "        # make white circle on a center\n",
    "        cv2.circle(mask, center, radius, (255, 255, 255), -1)\n",
    "    \n",
    "        mask = mask.astype(float) / 255.0\n",
    "        \n",
    "        fill_color = [0.485*255, 0.456*255, 0.406*255]\n",
    "        fill_color = np.array(fill_color, dtype=np.float32)\n",
    "        \n",
    "        fill_img = np.ones_like(img, dtype=np.float32) * fill_color\n",
    "    \n",
    "        result = img.astype(float) * mask[..., None] + fill_img * (1 - mask[..., None])\n",
    "    \n",
    "        return result.astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    def process_crops(self, crops):\n",
    "        processed = {}\n",
    "        \n",
    "        for key, img in crops.items():\n",
    "            # Skip text panel for CNN\n",
    "            if key == \"text_panel\":\n",
    "                processed[key] = img\n",
    "                continue\n",
    "            \n",
    "            processed[key] = self._preprocess_for_cnn(img)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    \n",
    "    def _preprocess_for_cnn(self, cropped_img):\n",
    "        # resize\n",
    "        size = self.configuration[\"resize\"]\n",
    "        img = cv2.resize(cropped_img, size)\n",
    "    \n",
    "        # convert to float32 0–1\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "        # ImageNet norm (change if you use custom model)\n",
    "        mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "        std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    \n",
    "        img = (img - mean) / std\n",
    "    \n",
    "        # HWC → CHW\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "    \n",
    "        return img\n",
    "    \n",
    "    \n",
    "    def extract_deep_features(self, tensor, save_dir=None, img_name=None, quadrant=None):\n",
    "\n",
    "        t = torch.from_numpy(tensor).unsqueeze(0).float().to(self.device)\n",
    "        # shape (1,3,224,224) for resnet 50\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            deep_features = self.feature_extraction_model(t).cpu().numpy().flatten()\n",
    "            # === OPTIONAL SAVE ===\n",
    "        if save_dir is not None:\n",
    "    \n",
    "            # Ensure folder exists\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "            # Build filename\n",
    "            # Example: \"image001_Q1.npy\"\n",
    "            if img_name is None:\n",
    "                img_name = \"unknown\"\n",
    "    \n",
    "            if quadrant is None:\n",
    "                quadrant = \"QX\"\n",
    "    \n",
    "            filename = f\"{img_name}_{quadrant}.npy\"\n",
    "            output_path = os.path.join(save_dir, filename)\n",
    "    \n",
    "            # Save features\n",
    "            np.save(output_path, deep_features)\n",
    "        return deep_features\n",
    "    \n",
    "\n",
    "    def handcrafted_features(self, cropped_img_pth, save_dir=None, img_name=None, quadrant=None):\n",
    "            \n",
    "        colors_platte = ColorThief(cropped_img_pth)\n",
    "        dominant_colors = colors_platte.get_palette(color_count=2)\n",
    "        # print(dominant_colors)\n",
    "        \n",
    "        ignored_colors = [\n",
    "            (0, 0, 0),\n",
    "            (255, 255, 255),\n",
    "            (int(0.485*255), int(0.456*255), int(0.406*255))  # fill mask base color\n",
    "        ]\n",
    "        tol = 15\n",
    "        cleaned_colors = []\n",
    "        \n",
    "        for R, G, B in dominant_colors:\n",
    "            bad = False\n",
    "            for ir, ig, ib in ignored_colors:\n",
    "                if (abs(R - ir) <= tol and\n",
    "                    abs(G - ig) <= tol and\n",
    "                    abs(B - ib) <= tol):\n",
    "                    bad = True\n",
    "                    break\n",
    "        \n",
    "            if not bad:\n",
    "                cleaned_colors.append((R, G, B))\n",
    "        \n",
    "        if len(cleaned_colors) == 0:\n",
    "            cleaned_colors = [(0,0,0)]\n",
    "        \n",
    "        R, G, B = cleaned_colors[0]\n",
    "        r, g, b = R/255.0, G/255.0, B/255.0\n",
    "        \n",
    "        h, s, v = colorsys.rgb_to_hsv(r, g, b)\n",
    "        \n",
    "        dom_h = h\n",
    "        dom_s = s\n",
    "        dom_v = v\n",
    "        \n",
    "        # ---- FIXED BGR → RGB for cv2 ----\n",
    "        cropped_img = cv2.imread(cropped_img_pth)\n",
    "        cropped_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        img = cropped_img.astype(np.float32) / 255.0\n",
    "        hsv = cv2.cvtColor(cropped_img, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "    \n",
    "        # Create mask of \"valid\" pixels matrix/grid\n",
    "        valid_mask = np.ones((img.shape[0], img.shape[1]), dtype=bool)\n",
    "    \n",
    "        for ir, ig, ib in ignored_colors:\n",
    "            if (ir, ig, ib) == ignored_colors[2]:\n",
    "                # This is the FILL MASK → use tolerance\n",
    "                invalid = (\n",
    "                    (np.abs(cropped_img[:,:,0] - ir) < tol) &\n",
    "                    (np.abs(cropped_img[:,:,1] - ig) < tol) &\n",
    "                    (np.abs(cropped_img[:,:,2] - ib) < tol)\n",
    "                )\n",
    "            else:\n",
    "                # Black & white → exact match\n",
    "                invalid = (\n",
    "                    (cropped_img[:,:,0] == ir) &\n",
    "                    (cropped_img[:,:,1] == ig) &\n",
    "                    (cropped_img[:,:,2] == ib)\n",
    "                )\n",
    "        \n",
    "            valid_mask[invalid] = False\n",
    "    \n",
    "        # Valid pixels only\n",
    "        valid_hsv = hsv[valid_mask]\n",
    "    \n",
    "        # Intensity Features (based on V channel)\n",
    "        V = valid_hsv[:,2] / 255.0\n",
    "    \n",
    "        avg_intensity = float(V.mean())\n",
    "        std_intensity = float(V.std())\n",
    "    \n",
    "        # Center vs Periphery intensities, the center sharpness\n",
    "        h_im, w_im = cropped_img.shape[:2]\n",
    "        cy, cx = h_im//2, w_im//2\n",
    "        R = min(cx, cy)\n",
    "    \n",
    "        r_center = int(R * 0.3)\n",
    "        r_mid = int(R * 0.60)\n",
    "    \n",
    "        Y, X = np.ogrid[:h_im, :w_im]\n",
    "        dist = np.sqrt((X - cx)**2 + (Y - cy)**2)\n",
    "    \n",
    "        center_mask = dist < r_center\n",
    "        periphery_mask = (dist > r_mid) & (dist < R)\n",
    "    \n",
    "        center_vals = img[center_mask][:, :].mean() if center_mask.any() else 0.0\n",
    "        periphery_vals = img[periphery_mask][:, :].mean() if periphery_mask.any() else 1e-6\n",
    "        \n",
    "        center_intensity = float(center_vals)\n",
    "        periphery_intensity = float(periphery_vals)\n",
    "        center_periphery_ratio = float(center_intensity / periphery_intensity)\n",
    "    \n",
    "        # Quadrant splits and asymmetry ratios\n",
    "        top = img[:h_im//2,:,:].mean()\n",
    "        bottom = img[h_im//2:,:,:].mean()\n",
    "        left = img[:, :w_im//2, :].mean()\n",
    "        right = img[:, w_im//2:, :].mean()\n",
    "    \n",
    "        inferior_superior_ratio = float(bottom / (top + 1e-6))\n",
    "        left_right_ratio = float(left / (right + 1e-6))\n",
    "    \n",
    "        diag1 = img[:h_im//2, :w_im//2, :].mean() - img[h_im//2:, w_im//2:, :].mean()\n",
    "        diag2 = img[:h_im//2, w_im//2:, :].mean() - img[h_im//2:, :w_im//2, :].mean()\n",
    "    \n",
    "        diag1_difference = float(diag1)\n",
    "        diag2_difference = float(diag2)\n",
    "    \n",
    "        radial_symmetry = float(\n",
    "            abs(top - bottom) +\n",
    "            abs(left - right) +\n",
    "            abs(diag1) +\n",
    "            abs(diag2)\n",
    "        )\n",
    "    \n",
    "        # ----- PACK RESULTS -----\n",
    "        features = {\n",
    "            \"dom_h\": dom_h,\n",
    "            \"dom_s\": dom_s,\n",
    "            \"dom_v\": dom_v,\n",
    "    \n",
    "            \"avg_intensity\": avg_intensity,\n",
    "            \"std_intensity\": std_intensity,\n",
    "    \n",
    "            \"center_intensity\": center_intensity,\n",
    "            \"periphery_intensity\": periphery_intensity,\n",
    "            \"center_periphery_ratio\": center_periphery_ratio,\n",
    "    \n",
    "            \"inferior_superior_ratio\": inferior_superior_ratio,\n",
    "            \"left_right_ratio\": left_right_ratio,\n",
    "    \n",
    "            \"diag1_difference\": diag1_difference,\n",
    "            \"diag2_difference\": diag2_difference,\n",
    "    \n",
    "            \"radial_symmetry\": radial_symmetry\n",
    "        }\n",
    "        self.handcraft_features = features\n",
    "    \n",
    "        # ============== OPTIONAL SAVE ==============\n",
    "        if save_dir is not None:\n",
    "    \n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "            if img_name is None:\n",
    "                img_name = \"unknown\"\n",
    "    \n",
    "            if quadrant is None:\n",
    "                quadrant = \"QX\"\n",
    "    \n",
    "            filename = f\"{img_name}_{quadrant}.json\"\n",
    "            output_path = os.path.join(save_dir, filename)\n",
    "    \n",
    "            # save handcrafted features as JSON\n",
    "            with open(output_path, \"w\") as f:\n",
    "                json.dump(features, f, indent=4)\n",
    "    \n",
    "        return features\n",
    "    \n",
    "      \n",
    "    def run_image_preprocessing(self, image_path):\n",
    "\n",
    "        # ---------------------------\n",
    "        # 0. Resolve save directories\n",
    "        # ---------------------------\n",
    "        class_dir = os.path.dirname(image_path)  # e.g. dataset/normal/images\n",
    "        base_dir = os.path.dirname(class_dir)    # e.g. dataset/normal\n",
    "    \n",
    "        save_deep_dir   = os.path.join(base_dir, \"deep_features\")\n",
    "        save_hand_dir   = os.path.join(base_dir, \"handcraft_features\")\n",
    "        save_text_dir   = os.path.join(base_dir, \"text_panels\")\n",
    "        save_crops_dir  = os.path.join(base_dir, \"crops\")\n",
    "    \n",
    "        os.makedirs(save_deep_dir, exist_ok=True)\n",
    "        os.makedirs(save_hand_dir, exist_ok=True)\n",
    "        os.makedirs(save_text_dir, exist_ok=True)\n",
    "        os.makedirs(save_crops_dir, exist_ok=True)\n",
    "    \n",
    "        img_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    \n",
    "        # ---------------------------\n",
    "        # 1. Load image as RGB\n",
    "        # ---------------------------\n",
    "        bgr = cv2.imread(image_path)\n",
    "        if bgr is None:\n",
    "            raise FileNotFoundError(f\"Could not read: {image_path}\")\n",
    "    \n",
    "        img = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        # ---------------------------\n",
    "        # 2. Crop into Q1–Q4 + text panel\n",
    "        # ---------------------------\n",
    "        crops = self.crop(img)\n",
    "    \n",
    "        # Save all crops for debugging / training\n",
    "        for key, crop_rgb in crops.items():\n",
    "            if key == \"text_panel\":\n",
    "                continue\n",
    "            else:\n",
    "                out_path = os.path.join(save_crops_dir, f\"{img_name}_{key}.png\")\n",
    "                cv2.imwrite(out_path, cv2.cvtColor(crop_rgb, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "        # ---------------------------\n",
    "        # 3. Prepare CNN tensors (skip text_panel)\n",
    "        # ---------------------------\n",
    "        processed = self.process_crops(crops)\n",
    "    \n",
    "        # ---------------------------\n",
    "        # 4. Process quadrants fully\n",
    "        # ---------------------------\n",
    "    \n",
    "        for key in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
    "    \n",
    "            crop_rgb = crops[key]\n",
    "            cnn_tensor = processed[key]\n",
    "    \n",
    "            # 4.1 deep features\n",
    "            deep = self.extract_deep_features(\n",
    "                tensor=cnn_tensor,\n",
    "                save_dir=save_deep_dir,\n",
    "                img_name=img_name,\n",
    "                quadrant=key\n",
    "            )\n",
    "    \n",
    "            # 4.2 handcrafted features — but we need a real file path\n",
    "            crop_file = os.path.join(save_crops_dir, f\"{img_name}_{key}.png\")\n",
    "            self.handcrafted_features(\n",
    "                cropped_img_pth=crop_file,\n",
    "                save_dir=save_hand_dir,\n",
    "                img_name=img_name,\n",
    "                quadrant=key\n",
    "            )\n",
    "    \n",
    "    \n",
    "        # ---------------------------\n",
    "        # 5. Save text panel for OCR\n",
    "        # ---------------------------\n",
    "        text_crop_rgb = crops[\"text_panel\"]\n",
    "        text_path = os.path.join(save_text_dir, f\"{img_name}_textpanel.png\")\n",
    "    \n",
    "        cv2.imwrite(text_path, cv2.cvtColor(text_crop_rgb, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "        return \n",
    "    \n",
    "\n",
    "    def run_vision_preprocessing(self, folder_path):\n",
    "\n",
    "        exts = (\".jpg\", \".jpeg\", \".png\", \".bmp\")\n",
    "        all_images = [\n",
    "            os.path.join(folder_path, f)\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.lower().endswith(exts)\n",
    "        ]\n",
    "    \n",
    "        print(f\"[INFO] Found {len(all_images)} images in: {folder_path}\")\n",
    "    \n",
    "       # parent class folder (normal or Keratoconus)\n",
    "        class_dir = os.path.dirname(folder_path)\n",
    "    \n",
    "        # resolved main output dirs\n",
    "        deep_dir = os.path.join(class_dir, \"deep_features\")\n",
    "        hand_dir = os.path.join(class_dir, \"handcraft_features\")\n",
    "        text_panel_dir = os.path.join(class_dir, \"text_panels\")\n",
    "        \n",
    "        processed_list = []\n",
    "\n",
    "        for img_path in all_images:\n",
    "            try:\n",
    "                print(f\"[INFO] Processing: {img_path}\")\n",
    "                self.run_image_preprocessing(img_path)\n",
    "                \n",
    "                processed_list.append(os.path.basename(img_path))\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed on {img_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "        print(\"[INFO] Completed folder processing.\")\n",
    "        \n",
    "        return {\n",
    "            \"deep_features_dir\": deep_dir,\n",
    "            \"handcraft_features_dir\": hand_dir,\n",
    "            \"text_panel_dir\": text_panel_dir,\n",
    "            \n",
    "            \"processed_images\": processed_list\n",
    "        }\n",
    "    \n",
    "\n",
    "    def run_vision_training(self, outputs, save_path=\"saved_models\"):\n",
    "    \n",
    "    \n",
    "        X = []\n",
    "        y = []\n",
    "    \n",
    "        label_map = self.label_map\n",
    "    \n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "        # ------------------------------------------\n",
    "        # LOOP OVER CLASSES\n",
    "        # ------------------------------------------\n",
    "        for class_name, info in outputs.items():\n",
    "    \n",
    "            class_label = label_map[class_name]\n",
    "    \n",
    "            deep_dir = info[\"deep_features_dir\"]\n",
    "            hand_dir = info[\"handcraft_features_dir\"]\n",
    "            processed_list = info[\"processed_images\"]\n",
    "    \n",
    "            # FUSION METADATA OUTPUT DIR\n",
    "            fusion_out_dir = os.path.join(\n",
    "                os.path.dirname(deep_dir), \"metadata_fusion\"\n",
    "            )\n",
    "            os.makedirs(fusion_out_dir, exist_ok=True)\n",
    "    \n",
    "            print(f\"[INFO] Generating fusion metadata for: {class_name}\")\n",
    "    \n",
    "            # ------------------------------------------\n",
    "            # PROCESS EACH IMAGE\n",
    "            # ------------------------------------------\n",
    "            for img_id in processed_list:\n",
    "                \n",
    "                img_id = os.path.splitext(img_id)[0]\n",
    "\n",
    "                quad_vecs = []\n",
    "                quad_hand_dict = {}\n",
    "    \n",
    "                # -----------------------\n",
    "                # load quadrant features\n",
    "                # -----------------------\n",
    "                for q in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
    "    \n",
    "                    npy_path = os.path.join(deep_dir, f\"{img_id}_{q}.npy\")\n",
    "                    j_path  = os.path.join(hand_dir, f\"{img_id}_{q}.json\")\n",
    "    \n",
    "                    # load deep features\n",
    "                    deep = np.load(npy_path).astype(np.float32)\n",
    "    \n",
    "                    # load handcrafted\n",
    "                    with open(j_path,\"r\") as f:\n",
    "                        hand = json.load(f)\n",
    "    \n",
    "                    # handcrafted vector\n",
    "                    hand_vec = np.array([\n",
    "                        hand[\"dom_h\"], hand[\"dom_s\"], hand[\"dom_v\"],\n",
    "                        hand[\"avg_intensity\"], hand[\"std_intensity\"],\n",
    "                        hand[\"center_intensity\"], hand[\"periphery_intensity\"],\n",
    "                        hand[\"center_periphery_ratio\"],\n",
    "                        hand[\"inferior_superior_ratio\"],\n",
    "                        hand[\"left_right_ratio\"],\n",
    "                        hand[\"diag1_difference\"],\n",
    "                        hand[\"diag2_difference\"],\n",
    "                        hand[\"radial_symmetry\"]\n",
    "                    ], dtype=np.float32)\n",
    "    \n",
    "                    quad_vecs.append(np.concatenate([deep, hand_vec], axis=0))\n",
    "                    quad_hand_dict[q] = hand\n",
    "    \n",
    "                # -------------------------\n",
    "                # FUSE quadrants\n",
    "                # -------------------------\n",
    "                fused_vec = np.mean(np.stack(quad_vecs), axis=0)\n",
    "    \n",
    "                X.append(fused_vec)\n",
    "                y.append(class_label)\n",
    "    \n",
    "                # -------------------------\n",
    "                # TEMPORARY fake prediction\n",
    "                # (during training we don't know yet)\n",
    "                # -------------------------\n",
    "                fusion_metadata = {\n",
    "                    \"class_name\": class_name,\n",
    "                    \"embedding\": fused_vec.tolist(),\n",
    "                    \"Qs\": quad_hand_dict,\n",
    "                }\n",
    "    \n",
    "                # save file\n",
    "                fusion_path = os.path.join(fusion_out_dir, f\"{img_id}.json\")\n",
    "                with open(fusion_path, \"w\") as f:\n",
    "                    json.dump(fusion_metadata, f, indent=4)\n",
    "    \n",
    "            print(f\"[INFO] Fusion metadata saved → {fusion_out_dir}\")\n",
    "    \n",
    "        # ------------------------------------------\n",
    "        # TRAIN CLASSIFIER\n",
    "        # ------------------------------------------\n",
    "        X = np.stack(X)\n",
    "        y = np.array(y)\n",
    "    \n",
    "        print(f\"[INFO] Training classifier on {X.shape[0]} samples, dim={X.shape[1]}\")\n",
    "    \n",
    "        self.classifier.fit(X, y)\n",
    "    \n",
    "        # ------------------------------------------\n",
    "        # SAVE MODELS\n",
    "        # ------------------------------------------\n",
    "        model_path = os.path.join(save_path, \"xgboost_vision_model.json\")\n",
    "        self.classifier.save(model_path)\n",
    "\n",
    "        torch.save(self.feature_extraction_model.state_dict(), os.path.join(save_path, \"feature_extractor.pth\"))\n",
    "    \n",
    "        print(\"[INFO] Models saved successfully.\")\n",
    "    \n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def vision_test(self, image_pth, save_path, model_dir=\"saved_models\"):\n",
    "        \n",
    "        if save_path is not None:\n",
    "            # Ensure parent folder exists\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "            # Subfolders\n",
    "            crops_save_dir = os.path.join(save_path, \"crops\")\n",
    "            hand_save_dir  = os.path.join(save_path, \"handcraft_features\")\n",
    "        \n",
    "            os.makedirs(crops_save_dir, exist_ok=True)\n",
    "            os.makedirs(hand_save_dir, exist_ok=True)\n",
    "        \n",
    "            # Extract image name without extension\n",
    "            img_name = os.path.splitext(os.path.basename(image_pth))[0]\n",
    "        \n",
    "        else:\n",
    "            crops_save_dir = None\n",
    "            hand_save_dir  = None\n",
    "            img_name = None\n",
    "\n",
    "            \n",
    "        # -------------------------------------------------\n",
    "        # 1. Load image\n",
    "        # -------------------------------------------------\n",
    "        img = cv2.imread(image_pth)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"[ERROR] Cannot load image: {image_pth}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 2. Crop quadrants\n",
    "        # -------------------------------------------------\n",
    "        crops = self.crop(img, img_name, crops_save_dir)\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 3. Preprocess quadrants for CNN (skip text panel)\n",
    "        # -------------------------------------------------\n",
    "        processed = self.process_crops(crops)\n",
    "    \n",
    "        quad_vecs = []\n",
    "        quad_output = {}\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 4. Deep + Handcrafted features\n",
    "        # -------------------------------------------------\n",
    "        for q in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
    "            tensor = processed[q]   # (3,224,224)\n",
    "            \n",
    "            if save_path is not None:\n",
    "                quadrant = q\n",
    "            else:\n",
    "                quadrant = None\n",
    "                \n",
    "            # deep features\n",
    "            deep_vec = self.extract_deep_features(\n",
    "                tensor=tensor,\n",
    "                save_dir=None,\n",
    "                img_name=None,\n",
    "                quadrant=None\n",
    "            )\n",
    "    \n",
    "            # handcrafted features\n",
    "            # first save crop to temp file for the ColorThief\n",
    "            tmp = os.path.join(model_dir, f\"_tmp_{q}.png\")\n",
    "            cv2.imwrite(tmp, cv2.cvtColor(crops[q], cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "            hand = self.handcrafted_features(\n",
    "                cropped_img_pth=tmp,\n",
    "                save_dir=hand_save_dir,\n",
    "                img_name=img_name,\n",
    "                quadrant=quadrant\n",
    "            )\n",
    "            os.remove(tmp)\n",
    "    \n",
    "            # convert handcraft dict → numeric vector\n",
    "            hand_vec = np.array([\n",
    "                hand[\"dom_h\"], hand[\"dom_s\"], hand[\"dom_v\"],\n",
    "                hand[\"avg_intensity\"], hand[\"std_intensity\"],\n",
    "                hand[\"center_intensity\"], hand[\"periphery_intensity\"],\n",
    "                hand[\"center_periphery_ratio\"],\n",
    "                hand[\"inferior_superior_ratio\"],\n",
    "                hand[\"left_right_ratio\"],\n",
    "                hand[\"diag1_difference\"],\n",
    "                hand[\"diag2_difference\"],\n",
    "                hand[\"radial_symmetry\"]\n",
    "            ], dtype=np.float32)\n",
    "    \n",
    "            # combine deep + hand\n",
    "            fused_q = np.concatenate([deep_vec, hand_vec], axis=0)\n",
    "            quad_vecs.append(fused_q)\n",
    "    \n",
    "            quad_output[q] = {\n",
    "                # \"deep_features\": deep_vec.tolist(),\n",
    "                \"handcrafted_features\": hand\n",
    "            }\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 5. Fuse quadrants (mean pooling)\n",
    "        # -------------------------------------------------\n",
    "        fused_embedding = np.mean(np.stack(quad_vecs), axis=0).reshape(1, -1)\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 6. Load XGBoost model\n",
    "        # -------------------------------------------------\n",
    "        model_path = os.path.join(model_dir, \"xgboost_vision_model.json\")\n",
    "        self.classifier.load(model_path)\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 7. Predict\n",
    "        # -------------------------------------------------\n",
    "        probs = self.classifier.predict_proba(fused_embedding)[0]\n",
    "        pred_idx = int(np.argmax(probs))\n",
    "    \n",
    "        # load metadata to invert mapping\n",
    "        pred_label = self.test_label_map[pred_idx]\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 8. Build return dict\n",
    "        # -------------------------------------------------\n",
    "        output = {\n",
    "            \"prediction\": pred_label,\n",
    "            \"confidence\": float(probs[pred_idx]),\n",
    "            \"probabilities\": probs.tolist(),\n",
    "            \"quadrant_features\": quad_output\n",
    "        }\n",
    "    \n",
    "        return output\n"
   ],
   "id": "199bcba3ab306a9d",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### create the backbone feature extractor",
   "id": "aa3f51c2b14108d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T19:37:15.747794Z",
     "start_time": "2025-12-03T19:37:14.864172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "feature_extractor = nn.Sequential(*list(backbone.children())[:-1])\n",
    "feature_extractor = feature_extractor.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "dcde4263e8fbbef8",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### create the head classifier",
   "id": "c53204dd568866d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T19:37:15.751991Z",
     "start_time": "2025-12-03T19:37:15.748797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "classifier = XGBoostVisionClassifier()\n"
   ],
   "id": "67f16640ef51214f",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### run the images preprocessing for later training",
   "id": "be9fa124a6535c2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:09:23.331264Z",
     "start_time": "2025-12-03T20:09:23.327321Z"
    }
   },
   "cell_type": "code",
   "source": "vision_obj = VisionModule(feature_extraction_model=feature_extractor,classifier=classifier, configuration=configuration)",
   "id": "4ad37c86f6e22e5f",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T19:37:34.754277Z",
     "start_time": "2025-12-03T19:37:15.758015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_paths = [r\"dataset/Keratoconus/images\", r\"dataset/normal/images\"] \n",
    "\n",
    "outputs={}\n",
    "\n",
    "for dataset_path in dataset_paths:\n",
    "    class_name = os.path.basename(os.path.dirname(dataset_path))\n",
    "    outputs[class_name] = vision_obj.run_vision_preprocessing(dataset_path)\n"
   ],
   "id": "f464e43d167dc7ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 10 images in: dataset/Keratoconus/images\n",
      "[INFO] Processing: dataset/Keratoconus/images\\53.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\55.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\56.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\57.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\59.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\61.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\62.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\65.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\66.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\67.jpg\n",
      "[INFO] Completed folder processing.\n",
      "[INFO] Found 10 images in: dataset/normal/images\n",
      "[INFO] Processing: dataset/normal/images\\207.jpg\n",
      "[INFO] Processing: dataset/normal/images\\208.jpg\n",
      "[INFO] Processing: dataset/normal/images\\209.jpg\n",
      "[INFO] Processing: dataset/normal/images\\210.jpg\n",
      "[INFO] Processing: dataset/normal/images\\211.jpg\n",
      "[INFO] Processing: dataset/normal/images\\212.jpg\n",
      "[INFO] Processing: dataset/normal/images\\213.jpg\n",
      "[INFO] Processing: dataset/normal/images\\214.jpg\n",
      "[INFO] Processing: dataset/normal/images\\215.jpg\n",
      "[INFO] Processing: dataset/normal/images\\216.jpg\n",
      "[INFO] Completed folder processing.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T19:37:35.254565Z",
     "start_time": "2025-12-03T19:37:34.756289Z"
    }
   },
   "cell_type": "code",
   "source": "vision_obj.run_vision_training(outputs=outputs)",
   "id": "7c491d13a5fa0091",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating fusion metadata for: Keratoconus\n",
      "[INFO] Fusion metadata saved → dataset/Keratoconus\\metadata_fusion\n",
      "[INFO] Generating fusion metadata for: normal\n",
      "[INFO] Fusion metadata saved → dataset/normal\\metadata_fusion\n",
      "[INFO] Training classifier on 20 samples, dim=2061\n",
      "[INFO] Models saved successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test the XGBOOST model",
   "id": "c8b5a115caf8a77e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:09:29.282801Z",
     "start_time": "2025-12-03T20:09:27.846278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_pth = r\"dataset/test/images/4.jpg\"\n",
    "result = vision_obj.vision_test(img_pth, save_path=\"dataset/test\")\n",
    "print(result)"
   ],
   "id": "354c8474f4a3daef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'normal', 'confidence': 0.6809717416763306, 'probabilities': [0.6809717416763306, 0.3190282881259918], 'quadrant_features': {'Q1': {'handcrafted_features': {'dom_h': 0.5438596491228069, 'dom_s': 0.1544715447154472, 'dom_v': 0.4823529411764706, 'avg_intensity': 0.934184193611145, 'std_intensity': 0.14357329905033112, 'center_intensity': 0.5666760206222534, 'periphery_intensity': 0.48184463381767273, 'center_periphery_ratio': 1.176055476912669, 'inferior_superior_ratio': 1.055221438407898, 'left_right_ratio': 0.9882144927978516, 'diag1_difference': -0.03271779417991638, 'diag2_difference': -0.020892947912216187, 'radial_symmetry': 0.08632847666740417}}, 'Q2': {'handcrafted_features': {'dom_h': 0.11917098445595854, 'dom_s': 0.8654708520179372, 'dom_v': 0.8745098039215686, 'avg_intensity': 0.8550441861152649, 'std_intensity': 0.16217155754566193, 'center_intensity': 0.5038502812385559, 'periphery_intensity': 0.48513951897621155, 'center_periphery_ratio': 1.0385677965419713, 'inferior_superior_ratio': 0.9605582356452942, 'left_right_ratio': 0.9780601859092712, 'diag1_difference': 0.008699536323547363, 'diag2_difference': 0.03007832169532776, 'radial_symmetry': 0.06885617971420288}}, 'Q3': {'handcrafted_features': {'dom_h': 0.5175438596491229, 'dom_s': 0.1557377049180328, 'dom_v': 0.47843137254901963, 'avg_intensity': 0.9052971601486206, 'std_intensity': 0.14224769175052643, 'center_intensity': 0.5106144547462463, 'periphery_intensity': 0.49411848187446594, 'center_periphery_ratio': 1.0333846505987836, 'inferior_superior_ratio': 0.9848600625991821, 'left_right_ratio': 0.9884722232818604, 'diag1_difference': 0.001802891492843628, 'diag2_difference': 0.013220608234405518, 'radial_symmetry': 0.02824416756629944}}, 'Q4': {'handcrafted_features': {'dom_h': 0.6896551724137931, 'dom_s': 0.22480620155038758, 'dom_v': 0.5058823529411764, 'avg_intensity': 0.8541324734687805, 'std_intensity': 0.19289858639240265, 'center_intensity': 0.46565791964530945, 'periphery_intensity': 0.45547860860824585, 'center_periphery_ratio': 1.0223486039622527, 'inferior_superior_ratio': 1.043607234954834, 'left_right_ratio': 0.9997486472129822, 'diag1_difference': -0.01998928189277649, 'diag2_difference': -0.019757002592086792, 'radial_symmetry': 0.05973553657531738}}}}\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# class TextModule:\n",
    "#     def __init__(self, ocr_engine, text_classifier, embed_compressor):\n",
    "#         self.ocr = ocr_engine\n",
    "#         self.text_classifier = text_classifier\n",
    "#         self.embed_compressor = embed_compressor\n",
    "# \n",
    "#     def ocr_extract(self, crop): ...\n",
    "#     def clean_text(self, raw): ...\n",
    "#     def extract_numeric(self, text): ...\n",
    "#     def classify_text(self, text): ...\n",
    "#     def build_output_dict(self, ...): ...\n",
    "# \n",
    "#     def run(self, OCR_crop):\n",
    "#         raw = self.ocr_extract(OCR_crop)\n",
    "#         cleaned = self.clean_text(raw)\n",
    "#         numeric = self.extract_numeric(cleaned)\n",
    "#         text_pred, text_probs, embed = self.classify_text(cleaned)\n",
    "# \n",
    "#         # build Text_Output_Dict\n",
    "#         return Text_Output_Dict\n"
   ],
   "id": "24ec2fe2097b2143",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T19:37:35.342411Z",
     "start_time": "2025-12-03T19:37:35.342411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class FusionModule:\n",
    "#     def __init__(self, xgb_model):\n",
    "#         self.model = xgb_model\n",
    "# \n",
    "#     def build_fusion_vector(self, image_dict, text_dict): ...\n",
    "#     def run(self, Image_Output_Dict, Text_Output_Dict):\n",
    "#         x = self.build_fusion_vector(Image_Output_Dict, Text_Output_Dict)\n",
    "#         pred = self.model.predict(x)\n",
    "#         conf = self.model.predict_proba(x)\n",
    "#         return Fusion_Output_Dict\n"
   ],
   "id": "91e3cc60629fccf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# class ReportModule:\n",
    "#     def __init__(self, t5_model, tokenizer):\n",
    "#         self.model = t5_model\n",
    "#         self.tokenizer = tokenizer\n",
    "# \n",
    "#     def build_long_text(self, image_dict, text_dict, fusion_dict): ...\n",
    "#     def summarize(self, text): ...\n",
    "# \n",
    "#     def run(self, Image_Output_Dict, Text_Output_Dict, Fusion_Output_Dict):\n",
    "#         full = self.build_long_text(...)\n",
    "#         summary = self.summarize(full)\n",
    "#         return summary\n"
   ],
   "id": "9eab50940608515b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def run_pipeline(image_path):\n",
    "#     # 1 Vision\n",
    "#     Image_Output_Dict, OCR_crop = VisionModule.run(image_path)\n",
    "# \n",
    "#     # 2 Text\n",
    "#     Text_Output_Dict = TextModule.run(OCR_crop)\n",
    "# \n",
    "#     # 3 Fusion\n",
    "#     Fusion_Output_Dict = FusionModule.run(Image_Output_Dict, Text_Output_Dict)\n",
    "# \n",
    "#     # 4 Report\n",
    "#     final_report = ReportModule.run(Image_Output_Dict, Text_Output_Dict, Fusion_Output_Dict)\n",
    "# \n",
    "#     return {\n",
    "#         \"vision\": Image_Output_Dict,\n",
    "#         \"text\": Text_Output_Dict,\n",
    "#         \"fusion\": Fusion_Output_Dict,\n",
    "#         \"report\": final_report\n",
    "#     }\n"
   ],
   "id": "e50156560e60733",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
