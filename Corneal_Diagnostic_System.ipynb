{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:27:46.642226Z",
     "start_time": "2025-12-04T13:27:46.636696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1200 * 910 the standers, pls edit below for test image dim\n",
    "configuration = {\n",
    "\n",
    "    \"resize\": (224, 224),\n",
    "\n",
    "    \"crop_positions\": {\n",
    "        \n",
    "        \"Q1\": {\"x1\": 772, \"y1\": 91, \"x2\": (772+360), \"y2\": (91+360), \"center\": (967,275), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q2\": {\"x1\": 410, \"y1\": 90, \"x2\": (410+360), \"y2\": (90+360), \"center\": (592, 275), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q3\": {\"x1\": 410, \"y1\": 467, \"x2\": (410+360), \"y2\": (467+360), \"center\": (592,652), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q4\": {\"x1\": 771, \"y1\": 467, \"x2\": (771+360), \"y2\": (467+360), \"center\": (967,652), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"text_panel\": {\"x1\": 10, \"y1\": 200, \"x2\": (10+315), \"y2\": (200+625)}\n",
    "    }\n",
    "}"
   ],
   "id": "d1a19ba5c7ccbc00",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:27:47.079322Z",
     "start_time": "2025-12-04T13:27:47.072066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "class XGBoostClassifier:\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_estimators=400,\n",
    "                 max_depth=6,\n",
    "                 learning_rate=0.05,\n",
    "                 subsample=0.9,\n",
    "                 colsample_bytree=0.9,\n",
    "                 random_state=42):\n",
    "\n",
    "        self.model = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    # ---------------------------------------\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    def save(self, path):\n",
    "        joblib.dump(self.model, path)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    def load(self, path):\n",
    "        self.model = joblib.load(path)\n",
    "        return self\n"
   ],
   "id": "ea59977f722e076",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:52:05.637819Z",
     "start_time": "2025-12-04T13:52:05.574284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import colorsys\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from colorthief import ColorThief\n",
    "\n",
    "\n",
    "class VisionModule:\n",
    "    \n",
    "    def __init__(self, feature_extraction_model, configuration):\n",
    "        self.handcraft_features = None\n",
    "        self.feature_extraction_model = feature_extraction_model\n",
    "        self.configuration = configuration\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.label_map = {  \n",
    "            \"normal\": 0,\n",
    "            \"Keratoconus\": 1\n",
    "        }\n",
    "        self.test_label_map = {  \n",
    "            0: \"normal\",\n",
    "            1: \"Keratoconus\"\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def crop(self, img, img_name=\"UNKNOWN\", save_dir=None):\n",
    "\n",
    "        crops = {}\n",
    "        crop_cfg = self.configuration[\"crop_positions\"]\n",
    "\n",
    "        for key, cfg in crop_cfg.items():\n",
    "            \n",
    "            # --- 1. Crop the region ---\n",
    "            x1, y1, x2, y2 = cfg[\"x1\"], cfg[\"y1\"], cfg[\"x2\"], cfg[\"y2\"]\n",
    "            crop_img = img[y1:y2, x1:x2].copy()\n",
    "            \n",
    "            # --- 2. Apply circular mask ONLY if mask_radius_factor exists ---\n",
    "            if cfg.get(\"apply_circular_mask\", False) is True:\n",
    "                radius = cfg.get(\"radius\", None)\n",
    "                center = cfg.get(\"center\", None)\n",
    "                center = (center[0] - x1, center[1] - y1)\n",
    "                crop_img = self._apply_circular_mask(\n",
    "                    crop_img,\n",
    "                    radius,\n",
    "                    center\n",
    "                )\n",
    "\n",
    "            # --- 3. Optionally save cropped image ---\n",
    "            if save_dir is not None and key != \"text_panel\":\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                save_path = os.path.join(save_dir, f\"{img_name}_{key}.png\")\n",
    "                cv2.imwrite(save_path, cv2.cvtColor(crop_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            # --- 4. Add to output dictionary ---\n",
    "            crops[key] = crop_img\n",
    "\n",
    "        return crops\n",
    "\n",
    "\n",
    "    def _apply_circular_mask(self, img, radius, center):\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        if center is not None:\n",
    "            center = (int(center[0]), int(center[1]))\n",
    "        else:\n",
    "            center = (int(w // 2), int(h // 2))\n",
    "            \n",
    "        if radius is not None:\n",
    "            radius = int(radius)\n",
    "        else:\n",
    "            radius = int(min(h, w) * 0.9 / 2)\n",
    "    \n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    \n",
    "        # make white circle on a center\n",
    "        cv2.circle(mask, center, radius, (255, 255, 255), -1)\n",
    "    \n",
    "        mask = mask.astype(float) / 255.0\n",
    "        \n",
    "        fill_color = [0.485*255, 0.456*255, 0.406*255]\n",
    "        fill_color = np.array(fill_color, dtype=np.float32)\n",
    "        \n",
    "        fill_img = np.ones_like(img, dtype=np.float32) * fill_color\n",
    "    \n",
    "        result = img.astype(float) * mask[..., None] + fill_img * (1 - mask[..., None])\n",
    "    \n",
    "        return result.astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    def process_crops(self, crops):\n",
    "        processed = {}\n",
    "        \n",
    "        for key, img in crops.items():\n",
    "            # Skip text panel for CNN\n",
    "            if key == \"text_panel\":\n",
    "                processed[key] = img\n",
    "                continue\n",
    "            \n",
    "            processed[key] = self._preprocess_for_cnn(img)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    \n",
    "    def preprocess_for_cnn(self, cropped_img):\n",
    "        # resize\n",
    "        size = self.configuration[\"resize\"]\n",
    "        img = cv2.resize(cropped_img, size)\n",
    "    \n",
    "        # convert to float32 0–1\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "        # ImageNet norm (change if you use custom model)\n",
    "        mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "        std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    \n",
    "        img = (img - mean) / std\n",
    "    \n",
    "        # HWC → CHW\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "    \n",
    "        return img\n",
    "    \n",
    "      \n",
    "    def run_image_preprocessing(self, image_path):\n",
    "\n",
    "        class_dir = os.path.dirname(image_path)  # e.g. dataset/normal/images\n",
    "        base_dir = os.path.dirname(class_dir)    # e.g. dataset/normal\n",
    "    \n",
    "        save_text_dir   = os.path.join(base_dir, \"text_panels\")\n",
    "        save_crops_dir  = os.path.join(base_dir, \"crops\")\n",
    "    \n",
    "        os.makedirs(save_text_dir, exist_ok=True)\n",
    "        os.makedirs(save_crops_dir, exist_ok=True)\n",
    "    \n",
    "        img_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    \n",
    "        bgr = cv2.imread(image_path)\n",
    "        if bgr is None:\n",
    "            raise FileNotFoundError(f\"Could not read: {image_path}\")\n",
    "    \n",
    "        img = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        crops = self.crop(img, img_name=img_name, save_dir=save_crops_dir)\n",
    "    \n",
    "        # Save all crops for debugging / training\n",
    "        for key, crop_rgb in crops.items():\n",
    "            if key == \"text_panel\":\n",
    "                continue\n",
    "            else:\n",
    "                out_path = os.path.join(save_crops_dir, f\"{img_name}_{key}.png\")\n",
    "                cv2.imwrite(out_path, cv2.cvtColor(crop_rgb, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "        # Save text panel for OCR\n",
    "        text_crop_rgb = crops[\"text_panel\"]\n",
    "        text_path = os.path.join(save_text_dir, f\"{img_name}_textpanel.png\")\n",
    "    \n",
    "        cv2.imwrite(text_path, cv2.cvtColor(text_crop_rgb, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "        return \n",
    "    \n",
    "\n",
    "    def run_vision_preprocessing(self, folder_path):\n",
    "\n",
    "        exts = (\".jpg\", \".jpeg\", \".png\", \".bmp\")\n",
    "        all_images = [\n",
    "            os.path.join(folder_path, f)\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.lower().endswith(exts)\n",
    "        ]\n",
    "    \n",
    "        print(f\"[INFO] Found {len(all_images)} images in: {folder_path}\")\n",
    "    \n",
    "       # parent class folder (normal or Keratoconus)\n",
    "        class_dir = os.path.dirname(folder_path)\n",
    "    \n",
    "        # resolved main output dirs\n",
    "        crops_dir = os.path.join(class_dir, \"crops\")\n",
    "        # deep_dir = os.path.join(class_dir, \"deep_features\")\n",
    "        # hand_dir = os.path.join(class_dir, \"handcraft_features\")\n",
    "        text_panel_dir = os.path.join(class_dir, \"text_panels\")\n",
    "        \n",
    "        processed_list = []\n",
    "\n",
    "        for img_path in all_images:\n",
    "            try:\n",
    "                print(f\"[INFO] Processing: {img_path}\")\n",
    "                self.run_image_preprocessing(img_path)\n",
    "                \n",
    "                processed_list.append(os.path.basename(img_path))\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed on {img_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "        print(\"[INFO] Completed folder processing.\")\n",
    "        \n",
    "        return {\n",
    "            \"crops\": crops_dir,\n",
    "            \"text_panel_dir\": text_panel_dir,\n",
    "            \"processed_images\": processed_list\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def extract_deep_features(self, tensor, save_dir=None, img_name=None, quadrant=None):\n",
    "\n",
    "        t = torch.from_numpy(tensor).unsqueeze(0).float().to(self.device)\n",
    "        # shape (1,3,224,224) for resnet 50\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            deep_features = self.feature_extraction_model(t).cpu().numpy().flatten()\n",
    "            # === OPTIONAL SAVE ===\n",
    "        if save_dir is not None:\n",
    "    \n",
    "            # Ensure folder exists\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "            # Build filename\n",
    "            # Example: \"image001_Q1.npy\"\n",
    "            if img_name is None:\n",
    "                img_name = \"unknown\"\n",
    "    \n",
    "            if quadrant is None:\n",
    "                quadrant = \"QX\"\n",
    "    \n",
    "            filename = f\"{img_name}_{quadrant}.npy\"\n",
    "            output_path = os.path.join(save_dir, filename)\n",
    "    \n",
    "            # Save features\n",
    "            np.save(output_path, deep_features)\n",
    "        return deep_features\n",
    "    \n",
    "\n",
    "    def handcrafted_features(self, cropped_img_pth, save_dir=None, img_name=None, quadrant=None):\n",
    "            \n",
    "        colors_platte = ColorThief(cropped_img_pth)\n",
    "        dominant_colors = colors_platte.get_palette(color_count=2)\n",
    "        # print(dominant_colors)\n",
    "        \n",
    "        ignored_colors = [\n",
    "            (0, 0, 0),\n",
    "            (255, 255, 255),\n",
    "            (int(0.485*255), int(0.456*255), int(0.406*255))  # fill mask base color\n",
    "        ]\n",
    "        tol = 15\n",
    "        cleaned_colors = []\n",
    "        \n",
    "        for R, G, B in dominant_colors:\n",
    "            bad = False\n",
    "            for ir, ig, ib in ignored_colors:\n",
    "                if (abs(R - ir) <= tol and\n",
    "                    abs(G - ig) <= tol and\n",
    "                    abs(B - ib) <= tol):\n",
    "                    bad = True\n",
    "                    break\n",
    "        \n",
    "            if not bad:\n",
    "                cleaned_colors.append((R, G, B))\n",
    "        \n",
    "        if len(cleaned_colors) == 0:\n",
    "            cleaned_colors = [(0,0,0)]\n",
    "        \n",
    "        R, G, B = cleaned_colors[0]\n",
    "        r, g, b = R/255.0, G/255.0, B/255.0\n",
    "        \n",
    "        h, s, v = colorsys.rgb_to_hsv(r, g, b)\n",
    "        \n",
    "        dom_h = h\n",
    "        dom_s = s\n",
    "        dom_v = v\n",
    "        \n",
    "        # ---- FIXED BGR → RGB for cv2 ----\n",
    "        cropped_img = cv2.imread(cropped_img_pth)\n",
    "        cropped_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        img = cropped_img.astype(np.float32) / 255.0\n",
    "        hsv = cv2.cvtColor(cropped_img, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "    \n",
    "        # Create mask of \"valid\" pixels matrix/grid\n",
    "        valid_mask = np.ones((img.shape[0], img.shape[1]), dtype=bool)\n",
    "    \n",
    "        for ir, ig, ib in ignored_colors:\n",
    "            if (ir, ig, ib) == ignored_colors[2]:\n",
    "                # This is the FILL MASK → use tolerance\n",
    "                invalid = (\n",
    "                    (np.abs(cropped_img[:,:,0] - ir) < tol) &\n",
    "                    (np.abs(cropped_img[:,:,1] - ig) < tol) &\n",
    "                    (np.abs(cropped_img[:,:,2] - ib) < tol)\n",
    "                )\n",
    "            else:\n",
    "                # Black & white → exact match\n",
    "                invalid = (\n",
    "                    (cropped_img[:,:,0] == ir) &\n",
    "                    (cropped_img[:,:,1] == ig) &\n",
    "                    (cropped_img[:,:,2] == ib)\n",
    "                )\n",
    "        \n",
    "            valid_mask[invalid] = False\n",
    "    \n",
    "        # Valid pixels only\n",
    "        valid_hsv = hsv[valid_mask]\n",
    "    \n",
    "        # Intensity Features (based on V channel)\n",
    "        V = valid_hsv[:,2] / 255.0\n",
    "    \n",
    "        avg_intensity = float(V.mean())\n",
    "        std_intensity = float(V.std())\n",
    "    \n",
    "        # Center vs Periphery intensities, the center sharpness\n",
    "        h_im, w_im = cropped_img.shape[:2]\n",
    "        cy, cx = h_im//2, w_im//2\n",
    "        R = min(cx, cy)\n",
    "    \n",
    "        r_center = int(R * 0.3)\n",
    "        r_mid = int(R * 0.60)\n",
    "    \n",
    "        Y, X = np.ogrid[:h_im, :w_im]\n",
    "        dist = np.sqrt((X - cx)**2 + (Y - cy)**2)\n",
    "    \n",
    "        center_mask = dist < r_center\n",
    "        periphery_mask = (dist > r_mid) & (dist < R)\n",
    "    \n",
    "        center_vals = img[center_mask][:, :].mean() if center_mask.any() else 0.0\n",
    "        periphery_vals = img[periphery_mask][:, :].mean() if periphery_mask.any() else 1e-6\n",
    "        \n",
    "        center_intensity = float(center_vals)\n",
    "        periphery_intensity = float(periphery_vals)\n",
    "        center_periphery_ratio = float(center_intensity / periphery_intensity)\n",
    "    \n",
    "        # Quadrant splits and asymmetry ratios\n",
    "        top = img[:h_im//2,:,:].mean()\n",
    "        bottom = img[h_im//2:,:,:].mean()\n",
    "        left = img[:, :w_im//2, :].mean()\n",
    "        right = img[:, w_im//2:, :].mean()\n",
    "    \n",
    "        inferior_superior_ratio = float(bottom / (top + 1e-6))\n",
    "        left_right_ratio = float(left / (right + 1e-6))\n",
    "    \n",
    "        diag1 = img[:h_im//2, :w_im//2, :].mean() - img[h_im//2:, w_im//2:, :].mean()\n",
    "        diag2 = img[:h_im//2, w_im//2:, :].mean() - img[h_im//2:, :w_im//2, :].mean()\n",
    "    \n",
    "        diag1_difference = float(diag1)\n",
    "        diag2_difference = float(diag2)\n",
    "    \n",
    "        radial_symmetry = float(\n",
    "            abs(top - bottom) +\n",
    "            abs(left - right) +\n",
    "            abs(diag1) +\n",
    "            abs(diag2)\n",
    "        )\n",
    "    \n",
    "        # ----- PACK RESULTS -----\n",
    "        features = {\n",
    "            \"dom_h\": dom_h,\n",
    "            \"dom_s\": dom_s,\n",
    "            \"dom_v\": dom_v,\n",
    "    \n",
    "            \"avg_intensity\": avg_intensity,\n",
    "            \"std_intensity\": std_intensity,\n",
    "    \n",
    "            \"center_intensity\": center_intensity,\n",
    "            \"periphery_intensity\": periphery_intensity,\n",
    "            \"center_periphery_ratio\": center_periphery_ratio,\n",
    "    \n",
    "            \"inferior_superior_ratio\": inferior_superior_ratio,\n",
    "            \"left_right_ratio\": left_right_ratio,\n",
    "    \n",
    "            \"diag1_difference\": diag1_difference,\n",
    "            \"diag2_difference\": diag2_difference,\n",
    "    \n",
    "            \"radial_symmetry\": radial_symmetry\n",
    "        }\n",
    "        self.handcraft_features = features\n",
    "    \n",
    "        # ============== OPTIONAL SAVE ==============\n",
    "        if save_dir is not None:\n",
    "    \n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "            if img_name is None:\n",
    "                img_name = \"unknown\"\n",
    "    \n",
    "            if quadrant is None:\n",
    "                quadrant = \"QX\"\n",
    "    \n",
    "            filename = f\"{img_name}_{quadrant}.json\"\n",
    "            output_path = os.path.join(save_dir, filename)\n",
    "    \n",
    "            # save handcrafted features as JSON\n",
    "            with open(output_path, \"w\") as f:\n",
    "                json.dump(features, f, indent=4)\n",
    "    \n",
    "        return features\n",
    "    \n",
    "\n",
    "    # def run_vision_training(self, outputs, save_path=\"saved_models\"):\n",
    "    # \n",
    "    # \n",
    "    #     X = []\n",
    "    #     y = []\n",
    "    # \n",
    "    #     label_map = self.label_map\n",
    "    # \n",
    "    #     os.makedirs(save_path, exist_ok=True)\n",
    "    # \n",
    "    #     # ------------------------------------------\n",
    "    #     # LOOP OVER CLASSES\n",
    "    #     # ------------------------------------------\n",
    "    #     for class_name, info in outputs.items():\n",
    "    # \n",
    "    #         class_label = label_map[class_name]\n",
    "    # \n",
    "    #         deep_dir = info[\"deep_features_dir\"]\n",
    "    #         hand_dir = info[\"handcraft_features_dir\"]\n",
    "    #         processed_list = info[\"processed_images\"]\n",
    "    # \n",
    "    #         # FUSION METADATA OUTPUT DIR\n",
    "    #         fusion_out_dir = os.path.join(\n",
    "    #             os.path.dirname(deep_dir), \"metadata_fusion\"\n",
    "    #         )\n",
    "    #         os.makedirs(fusion_out_dir, exist_ok=True)\n",
    "    # \n",
    "    #         print(f\"[INFO] Generating fusion metadata for: {class_name}\")\n",
    "    # \n",
    "    #         # ------------------------------------------\n",
    "    #         # PROCESS EACH IMAGE\n",
    "    #         # ------------------------------------------\n",
    "    #         for img_id in processed_list:\n",
    "    #             \n",
    "    #             img_id = os.path.splitext(img_id)[0]\n",
    "    # \n",
    "    #             quad_vecs = []\n",
    "    #             quad_hand_dict = {}\n",
    "    # \n",
    "    #             # -----------------------\n",
    "    #             # load quadrant features\n",
    "    #             # -----------------------\n",
    "    #             for q in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
    "    # \n",
    "    #                 npy_path = os.path.join(deep_dir, f\"{img_id}_{q}.npy\")\n",
    "    #                 j_path  = os.path.join(hand_dir, f\"{img_id}_{q}.json\")\n",
    "    # \n",
    "    #                 # load deep features\n",
    "    #                 deep = np.load(npy_path).astype(np.float32)\n",
    "    # \n",
    "    #                 # load handcrafted\n",
    "    #                 with open(j_path,\"r\") as f:\n",
    "    #                     hand = json.load(f)\n",
    "    # \n",
    "    #                 # handcrafted vector\n",
    "    #                 hand_vec = np.array([\n",
    "    #                     hand[\"dom_h\"], hand[\"dom_s\"], hand[\"dom_v\"],\n",
    "    #                     hand[\"avg_intensity\"], hand[\"std_intensity\"],\n",
    "    #                     hand[\"center_intensity\"], hand[\"periphery_intensity\"],\n",
    "    #                     hand[\"center_periphery_ratio\"],\n",
    "    #                     hand[\"inferior_superior_ratio\"],\n",
    "    #                     hand[\"left_right_ratio\"],\n",
    "    #                     hand[\"diag1_difference\"],\n",
    "    #                     hand[\"diag2_difference\"],\n",
    "    #                     hand[\"radial_symmetry\"]\n",
    "    #                 ], dtype=np.float32)\n",
    "    # \n",
    "    #                 quad_vecs.append(np.concatenate([deep, hand_vec], axis=0))\n",
    "    #                 quad_hand_dict[q] = hand\n",
    "    # \n",
    "    #             # -------------------------\n",
    "    #             # FUSE quadrants\n",
    "    #             # -------------------------\n",
    "    #             fused_vec = np.mean(np.stack(quad_vecs), axis=0)\n",
    "    # \n",
    "    #             X.append(fused_vec)\n",
    "    #             y.append(class_label)\n",
    "    # \n",
    "    #             # -------------------------\n",
    "    #             # TEMPORARY fake prediction\n",
    "    #             # (during training we don't know yet)\n",
    "    #             # -------------------------\n",
    "    #             fusion_metadata = {\n",
    "    #                 \"class_name\": class_name,\n",
    "    #                 \"embedding\": fused_vec.tolist(),\n",
    "    #                 \"Qs\": quad_hand_dict,\n",
    "    #             }\n",
    "    # \n",
    "    #             # save file\n",
    "    #             fusion_path = os.path.join(fusion_out_dir, f\"{img_id}.json\")\n",
    "    #             with open(fusion_path, \"w\") as f:\n",
    "    #                 json.dump(fusion_metadata, f, indent=4)\n",
    "    # \n",
    "    #         print(f\"[INFO] Fusion metadata saved → {fusion_out_dir}\")\n",
    "    # \n",
    "    #     # ------------------------------------------\n",
    "    #     # TRAIN CLASSIFIER\n",
    "    #     # ------------------------------------------\n",
    "    #     X = np.stack(X)\n",
    "    #     y = np.array(y)\n",
    "    # \n",
    "    #     print(f\"[INFO] Training classifier on {X.shape[0]} samples, dim={X.shape[1]}\")\n",
    "    # \n",
    "    #     self.classifier.fit(X, y)\n",
    "    # \n",
    "    #     # ------------------------------------------\n",
    "    #     # SAVE MODELS\n",
    "    #     # ------------------------------------------\n",
    "    #     model_path = os.path.join(save_path, \"xgboost_vision_model.json\")\n",
    "    #     self.classifier.save(model_path)\n",
    "    # \n",
    "    #     torch.save(self.feature_extraction_model.state_dict(), os.path.join(save_path, \"feature_extractor.pth\"))\n",
    "    # \n",
    "    #     print(\"[INFO] Models saved successfully.\")\n",
    "    # \n",
    "    #     return True\n",
    "    \n",
    "    def split(self):\n",
    "        pass\n",
    "    \n",
    "    # def vision_test(self, image_pth, save_path, model_dir=\"saved_models\"):\n",
    "    #     \n",
    "    #     if save_path is not None:\n",
    "    #         # Ensure parent folder exists\n",
    "    #         os.makedirs(save_path, exist_ok=True)\n",
    "    #     \n",
    "    #         # Subfolders\n",
    "    #         crops_save_dir = os.path.join(save_path, \"crops\")\n",
    "    #         hand_save_dir  = os.path.join(save_path, \"handcraft_features\")\n",
    "    #     \n",
    "    #         os.makedirs(crops_save_dir, exist_ok=True)\n",
    "    #         os.makedirs(hand_save_dir, exist_ok=True)\n",
    "    #     \n",
    "    #         # Extract image name without extension\n",
    "    #         img_name = os.path.splitext(os.path.basename(image_pth))[0]\n",
    "    #     \n",
    "    #     else:\n",
    "    #         crops_save_dir = None\n",
    "    #         hand_save_dir  = None\n",
    "    #         img_name = None\n",
    "    # \n",
    "    #         \n",
    "    #     # -------------------------------------------------\n",
    "    #     # 1. Load image\n",
    "    #     # -------------------------------------------------\n",
    "    #     img = cv2.imread(image_pth)\n",
    "    #     if img is None:\n",
    "    #         raise ValueError(f\"[ERROR] Cannot load image: {image_pth}\")\n",
    "    #     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # \n",
    "    #     # -------------------------------------------------\n",
    "    #     # 2. Crop quadrants\n",
    "    #     # -------------------------------------------------\n",
    "    #     crops = self.crop(img, img_name, crops_save_dir)\n",
    "    # \n",
    "    #     # -------------------------------------------------\n",
    "    #     # 3. Preprocess quadrants for CNN (skip text panel)\n",
    "    #     # -------------------------------------------------\n",
    "    #     processed = self.process_crops(crops)\n",
    "    # \n",
    "    #     quad_vecs = []\n",
    "    #     quad_output = {}\n",
    "    # \n",
    "    #     # -------------------------------------------------\n",
    "    #     # 4. Deep + Handcrafted features\n",
    "    #     # -------------------------------------------------\n",
    "    #     for q in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
    "    #         tensor = processed[q]   # (3,224,224)\n",
    "    #         \n",
    "    #         if save_path is not None:\n",
    "    #             quadrant = q\n",
    "    #         else:\n",
    "    #             quadrant = None\n",
    "    #             \n",
    "    #         # deep features\n",
    "    #         deep_vec = self.extract_deep_features(\n",
    "    #             tensor=tensor,\n",
    "    #             save_dir=None,\n",
    "    #             img_name=None,\n",
    "    #             quadrant=None\n",
    "    #         )\n",
    "    # \n",
    "    #         # handcrafted features\n",
    "    #         # first save crop to temp file for the ColorThief\n",
    "    #         tmp = os.path.join(model_dir, f\"_tmp_{q}.png\")\n",
    "    #         cv2.imwrite(tmp, cv2.cvtColor(crops[q], cv2.COLOR_RGB2BGR))\n",
    "    # \n",
    "    #         hand = self.handcrafted_features(\n",
    "    #             cropped_img_pth=tmp,\n",
    "    #             save_dir=hand_save_dir,\n",
    "    #             img_name=img_name,\n",
    "    #             quadrant=quadrant\n",
    "    #         )\n",
    "    #         os.remove(tmp)\n",
    "    # \n",
    "    #         # convert handcraft dict → numeric vector\n",
    "    #         hand_vec = np.array([\n",
    "    #             hand[\"dom_h\"], hand[\"dom_s\"], hand[\"dom_v\"],\n",
    "    #             hand[\"avg_intensity\"], hand[\"std_intensity\"],\n",
    "    #             hand[\"center_intensity\"], hand[\"periphery_intensity\"],\n",
    "    #             hand[\"center_periphery_ratio\"],\n",
    "    #             hand[\"inferior_superior_ratio\"],\n",
    "    #             hand[\"left_right_ratio\"],\n",
    "    #             hand[\"diag1_difference\"],\n",
    "    #             hand[\"diag2_difference\"],\n",
    "    #             hand[\"radial_symmetry\"]\n",
    "    #         ], dtype=np.float32)\n",
    "    # \n",
    "    #         # combine deep + hand\n",
    "    #         fused_q = np.concatenate([deep_vec, hand_vec], axis=0)\n",
    "    #         quad_vecs.append(fused_q)\n",
    "    # \n",
    "    #         quad_output[q] = {\n",
    "    #             # \"deep_features\": deep_vec.tolist(),\n",
    "    #             \"handcrafted_features\": hand\n",
    "    #         }\n",
    "    # \n",
    "    #     # -------------------------------------------------\n",
    "    #     # 5. Fuse quadrants (mean pooling)\n",
    "    #     # -------------------------------------------------\n",
    "    #     fused_embedding = np.mean(np.stack(quad_vecs), axis=0).reshape(1, -1)\n",
    "    # \n",
    "    #     # -------------------------------------------------\n",
    "    #     # 6. Load XGBoost model\n",
    "    #     # -------------------------------------------------\n",
    "    #     model_path = os.path.join(model_dir, \"xgboost_vision_model.json\")\n",
    "    #     self.classifier.load(model_path)\n",
    "    # \n",
    "    #     # -------------------------------------------------\n",
    "    #     # 7. Predict\n",
    "    #     # -------------------------------------------------\n",
    "    #     probs = self.classifier.predict_proba(fused_embedding)[0]\n",
    "    #     pred_idx = int(np.argmax(probs))\n",
    "    # \n",
    "    #     # load metadata to invert mapping\n",
    "    #     pred_label = self.test_label_map[pred_idx]\n",
    "    # \n",
    "    #     # -------------------------------------------------\n",
    "    #     # 8. Build return dict\n",
    "    #     # -------------------------------------------------\n",
    "    #     output = {\n",
    "    #         \"prediction\": pred_label,\n",
    "    #         \"confidence\": float(probs[pred_idx]),\n",
    "    #         \"probabilities\": probs.tolist(),\n",
    "    #         \"quadrant_features\": quad_output\n",
    "    #     }\n",
    "    # \n",
    "    #     return output\n"
   ],
   "id": "199bcba3ab306a9d",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### create the backbone feature extractor",
   "id": "aa3f51c2b14108d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:52:09.268996Z",
     "start_time": "2025-12-04T13:52:07.771999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "feature_extractor = nn.Sequential(*list(backbone.children())[:-1])\n",
    "feature_extractor = feature_extractor.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "dcde4263e8fbbef8",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:52:10.107330Z",
     "start_time": "2025-12-04T13:52:10.101329Z"
    }
   },
   "cell_type": "code",
   "source": "vision_obj = VisionModule(feature_extraction_model=feature_extractor, configuration=configuration)",
   "id": "4ad37c86f6e22e5f",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:27:57.186203Z",
     "start_time": "2025-12-04T13:27:55.399456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_paths = [r\"dataset/Keratoconus/images\", r\"dataset/normal/images\"] \n",
    "\n",
    "outputs={}\n",
    "\n",
    "for dataset_path in dataset_paths:\n",
    "    class_name = os.path.basename(os.path.dirname(dataset_path))\n",
    "    outputs[class_name] = vision_obj.run_vision_preprocessing(dataset_path)\n"
   ],
   "id": "f464e43d167dc7ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 10 images in: dataset/Keratoconus/images\n",
      "[INFO] Processing: dataset/Keratoconus/images\\53.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\55.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\56.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\57.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\59.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\61.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\62.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\65.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\66.jpg\n",
      "[INFO] Processing: dataset/Keratoconus/images\\67.jpg\n",
      "[INFO] Completed folder processing.\n",
      "[INFO] Found 10 images in: dataset/normal/images\n",
      "[INFO] Processing: dataset/normal/images\\207.jpg\n",
      "[INFO] Processing: dataset/normal/images\\208.jpg\n",
      "[INFO] Processing: dataset/normal/images\\209.jpg\n",
      "[INFO] Processing: dataset/normal/images\\210.jpg\n",
      "[INFO] Processing: dataset/normal/images\\211.jpg\n",
      "[INFO] Processing: dataset/normal/images\\212.jpg\n",
      "[INFO] Processing: dataset/normal/images\\213.jpg\n",
      "[INFO] Processing: dataset/normal/images\\214.jpg\n",
      "[INFO] Processing: dataset/normal/images\\215.jpg\n",
      "[INFO] Processing: dataset/normal/images\\216.jpg\n",
      "[INFO] Completed folder processing.\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:28:21.519346Z",
     "start_time": "2025-12-04T13:28:21.512537Z"
    }
   },
   "cell_type": "code",
   "source": "print(outputs)",
   "id": "8d20e45cabe1715",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Keratoconus': {'crops': 'dataset/Keratoconus\\\\crops', 'text_panel_dir': 'dataset/Keratoconus\\\\text_panels', 'processed_images': ['53.jpg', '55.jpg', '56.jpg', '57.jpg', '59.jpg', '61.jpg', '62.jpg', '65.jpg', '66.jpg', '67.jpg']}, 'normal': {'crops': 'dataset/normal\\\\crops', 'text_panel_dir': 'dataset/normal\\\\text_panels', 'processed_images': ['207.jpg', '208.jpg', '209.jpg', '210.jpg', '211.jpg', '212.jpg', '213.jpg', '214.jpg', '215.jpg', '216.jpg']}}\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:52:49.271342Z",
     "start_time": "2025-12-04T13:52:27.755169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for class_name, info in outputs.items():\n",
    "\n",
    "        print(f\"\\n[INFO] Processing class: {class_name}\")\n",
    "\n",
    "        crops_dir = info[\"crops\"]                   # where Q1/Q2/Q3/Q4 crops are\n",
    "        processed_images = info[\"processed_images\"] # list of filenames\n",
    "        class_base = os.path.dirname(crops_dir)     # ../normal or ../Keratoconus\n",
    "\n",
    "        # Create output dirs\n",
    "        deep_dir = os.path.join(class_base, \"deep_features\")\n",
    "        hand_dir = os.path.join(class_base, \"handcraft_features\")\n",
    "\n",
    "        os.makedirs(deep_dir, exist_ok=True)\n",
    "        os.makedirs(hand_dir, exist_ok=True)\n",
    "\n",
    "        for img_name in processed_images:\n",
    "\n",
    "            img_id = os.path.splitext(img_name)[0]\n",
    "\n",
    "            print(f\"[INFO] Extracting features for: {img_id}\")\n",
    "\n",
    "            for q in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
    "\n",
    "                crop_path = os.path.join(crops_dir, f\"{img_id}_{q}.png\")\n",
    "\n",
    "                if not os.path.exists(crop_path):\n",
    "                    print(f\"[WARN] Missing crop: {crop_path}\")\n",
    "                    continue\n",
    "\n",
    "                # -----------------------------\n",
    "                # Load crop\n",
    "                # -----------------------------\n",
    "                crop = cv2.imread(crop_path)\n",
    "                crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # -----------------------------\n",
    "                # Deep features (CNN)\n",
    "                # -----------------------------\n",
    "                cnn_ready = vision_obj.preprocess_for_cnn(crop)\n",
    "                deep_vec = vision_obj.extract_deep_features(\n",
    "                    tensor=cnn_ready,\n",
    "                    save_dir=deep_dir,\n",
    "                    img_name=img_id,\n",
    "                    quadrant=q\n",
    "                )\n",
    "\n",
    "                # -----------------------------\n",
    "                # Handcrafted features\n",
    "                # -----------------------------\n",
    "                vision_obj.handcrafted_features(\n",
    "                    cropped_img_pth=crop_path,\n",
    "                    save_dir=hand_dir,\n",
    "                    img_name=img_id,\n",
    "                    quadrant=q\n",
    "                )\n",
    "\n",
    "        print(f\"[INFO] Completed extracting features for class {class_name}\")"
   ],
   "id": "5d27df5fe62c1912",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing class: Keratoconus\n",
      "[INFO] Extracting features for: 53\n",
      "[INFO] Extracting features for: 55\n",
      "[INFO] Extracting features for: 56\n",
      "[INFO] Extracting features for: 57\n",
      "[INFO] Extracting features for: 59\n",
      "[INFO] Extracting features for: 61\n",
      "[INFO] Extracting features for: 62\n",
      "[INFO] Extracting features for: 65\n",
      "[INFO] Extracting features for: 66\n",
      "[INFO] Extracting features for: 67\n",
      "[INFO] Completed extracting features for class Keratoconus\n",
      "\n",
      "[INFO] Processing class: normal\n",
      "[INFO] Extracting features for: 207\n",
      "[INFO] Extracting features for: 208\n",
      "[INFO] Extracting features for: 209\n",
      "[INFO] Extracting features for: 210\n",
      "[INFO] Extracting features for: 211\n",
      "[INFO] Extracting features for: 212\n",
      "[INFO] Extracting features for: 213\n",
      "[INFO] Extracting features for: 214\n",
      "[INFO] Extracting features for: 215\n",
      "[INFO] Extracting features for: 216\n",
      "[INFO] Completed extracting features for class normal\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import easyocr\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "class TextModule:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 transformer_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                 device=None):\n",
    "\n",
    "        # Device\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        # OCR engine\n",
    "        self.reader = easyocr.Reader(['en'])\n",
    "\n",
    "        # Transformer encoder\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(transformer_model)\n",
    "        self.model = AutoModel.from_pretrained(transformer_model).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. OCR Extraction\n",
    "    # ---------------------------------------------------------\n",
    "    def extract_text(self, text_panel_img):\n",
    "        \"\"\"\n",
    "        Input: RGB image crop of text panel\n",
    "        Output: clean concatenated text\n",
    "        \"\"\"\n",
    "        result = self.reader.readtext(text_panel_img)\n",
    "        lines = [res[1] for res in result]\n",
    "        raw_text = \"\\n\".join(lines)\n",
    "        return raw_text.strip()\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Clinical Numeric Feature Extraction\n",
    "    # ---------------------------------------------------------\n",
    "    def extract_clinical_features(self, text):\n",
    "        \"\"\"\n",
    "        Extract important Pentacam numeric values using regex.\n",
    "        If not found, returns None.\n",
    "        \"\"\"\n",
    "\n",
    "        clinical = {}\n",
    "\n",
    "        patterns = {\n",
    "            \"k1\":  r\"K1[:\\s]*([0-9.]+)\",\n",
    "            \"k2\":  r\"K2[:\\s]*([0-9.]+)\",\n",
    "            \"kmax\": r\"Kmax[:\\s]*([0-9.]+)\",\n",
    "            \"astig\": r\"Astig[:\\s]*([0-9.\\-]+)\",\n",
    "            \"pachy\": r\"Pachy[:\\s]*([0-9]+)\",\n",
    "            \"thinnest\": r\"Thin[:\\s]*([0-9]+)\"\n",
    "        }\n",
    "\n",
    "        for key, pat in patterns.items():\n",
    "            match = re.search(pat, text, flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                clinical[key] = float(match.group(1))\n",
    "            else:\n",
    "                clinical[key] = None\n",
    "\n",
    "        return clinical\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. Transformer Text Embedding\n",
    "    # ---------------------------------------------------------\n",
    "    def text_embedding(self, text):\n",
    "        \"\"\"\n",
    "        Convert text into dense embedding vector (CLS token).\n",
    "        \"\"\"\n",
    "\n",
    "        if text.strip() == \"\":\n",
    "            return np.zeros(384, dtype=np.float32)  # embedding size for MiniLM\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        # CLS token embedding\n",
    "        emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "        return emb.astype(np.float32)\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. Complete text processing (OCR → clinical → embedding)\n",
    "    # ---------------------------------------------------------\n",
    "    def process_text_panel(self, text_panel_img, save_path=None, img_name=None):\n",
    "        \"\"\"\n",
    "        Input: RGB image (text panel crop)\n",
    "        Output: structured dict:\n",
    "        {\n",
    "           \"raw_text\": \"...\",\n",
    "           \"clinical_features\": {...},\n",
    "           \"text_embedding\": [...]\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Raw OCR text\n",
    "        raw_text = self.extract_text(text_panel_img)\n",
    "\n",
    "        # 2. Clinical numeric values\n",
    "        clinical_features = self.extract_clinical_features(raw_text)\n",
    "\n",
    "        # 3. Transformer text vector\n",
    "        embedding = self.text_embedding(raw_text)\n",
    "\n",
    "        output = {\n",
    "            \"raw_text\": raw_text,\n",
    "            \"clinical_features\": clinical_features,\n",
    "            \"text_embedding\": embedding.tolist()\n",
    "        }\n",
    "\n",
    "        # 4. Optional saving\n",
    "        if save_path is not None:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            if img_name is None:\n",
    "                img_name = \"unknown\"\n",
    "\n",
    "            out_file = os.path.join(save_path, f\"{img_name}_text.json\")\n",
    "            import json\n",
    "            with open(out_file, \"w\") as f:\n",
    "                json.dump(output, f, indent=4)\n",
    "\n",
    "        return output\n"
   ],
   "id": "24ec2fe2097b2143",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T19:37:35.342411Z",
     "start_time": "2025-12-03T19:37:35.342411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class FusionModule:\n",
    "#     def __init__(self, xgb_model):\n",
    "#         self.model = xgb_model\n",
    "# \n",
    "#     def build_fusion_vector(self, image_dict, text_dict): ...\n",
    "#     def run(self, Image_Output_Dict, Text_Output_Dict):\n",
    "#         x = self.build_fusion_vector(Image_Output_Dict, Text_Output_Dict)\n",
    "#         pred = self.model.predict(x)\n",
    "#         conf = self.model.predict_proba(x)\n",
    "#         return Fusion_Output_Dict\n"
   ],
   "id": "91e3cc60629fccf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# class ReportModule:\n",
    "#     def __init__(self, t5_model, tokenizer):\n",
    "#         self.model = t5_model\n",
    "#         self.tokenizer = tokenizer\n",
    "# \n",
    "#     def build_long_text(self, image_dict, text_dict, fusion_dict): ...\n",
    "#     def summarize(self, text): ...\n",
    "# \n",
    "#     def run(self, Image_Output_Dict, Text_Output_Dict, Fusion_Output_Dict):\n",
    "#         full = self.build_long_text(...)\n",
    "#         summary = self.summarize(full)\n",
    "#         return summary\n"
   ],
   "id": "9eab50940608515b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def run_pipeline(image_path):\n",
    "#     # 1 Vision\n",
    "#     Image_Output_Dict, OCR_crop = VisionModule.run(image_path)\n",
    "# \n",
    "#     # 2 Text\n",
    "#     Text_Output_Dict = TextModule.run(OCR_crop)\n",
    "# \n",
    "#     # 3 Fusion\n",
    "#     Fusion_Output_Dict = FusionModule.run(Image_Output_Dict, Text_Output_Dict)\n",
    "# \n",
    "#     # 4 Report\n",
    "#     final_report = ReportModule.run(Image_Output_Dict, Text_Output_Dict, Fusion_Output_Dict)\n",
    "# \n",
    "#     return {\n",
    "#         \"vision\": Image_Output_Dict,\n",
    "#         \"text\": Text_Output_Dict,\n",
    "#         \"fusion\": Fusion_Output_Dict,\n",
    "#         \"report\": final_report\n",
    "#     }\n"
   ],
   "id": "e50156560e60733",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
