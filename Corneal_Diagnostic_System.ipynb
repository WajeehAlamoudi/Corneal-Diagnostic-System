{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# class ImageClassifier(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim=64, num_classes=5):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "#         \n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "# torch.save(image_classifier.state_dict(), \"image_classifier.pt\")\n"
   ],
   "id": "ea59977f722e076"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class VisionModule:\n",
    "#     def __init__(self, resnet_model, image_classifier):\n",
    "#         self.resnet = resnet_model\n",
    "#         self.image_classifier = image_classifier\n",
    "# \n",
    "#     def load_image(self, path): ...\n",
    "#     def extract_deep_features(self, q): ...\n",
    "#     def extract_handcrafted(self, q): ...\n",
    "#     def build_output_dict(self, ...): ...\n",
    "# \n",
    "#     def run(self, image_path):\n",
    "#         img = self.load_image(image_path)\n",
    "#         Q1, Q2, Q3, Q4, OCR_crop = self.crop_quadrants(img)\n",
    "# \n",
    "#         # run all quadrant processing\n",
    "#         # build Image_Output_Dict\n",
    "#         return Image_Output_Dict, OCR_crop\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T13:56:14.707939Z",
     "start_time": "2025-12-03T13:56:14.702853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "configuration = {\n",
    "\n",
    "    \"resize\": (224, 224),\n",
    "\n",
    "    \"crop_positions\": {\n",
    "        \n",
    "        \"Q1\": {\"x1\": 772, \"y1\": 91, \"x2\": (772+360), \"y2\": (91+360), \"center\": (967,275), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q2\": {\"x1\": 410, \"y1\": 90, \"x2\": (410+360), \"y2\": (90+360), \"center\": (592, 275), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q3\": {\"x1\": 410, \"y1\": 467, \"x2\": (410+360), \"y2\": (467+360), \"center\": (592,652), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q4\": {\"x1\": 771, \"y1\": 467, \"x2\": (771+360), \"y2\": (467+360), \"center\": (967,652), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"text_panel\": {\"x1\": 10, \"y1\": 200, \"x2\": (10+315), \"y2\": (200+625)}\n",
    "    }\n",
    "}"
   ],
   "id": "d1a19ba5c7ccbc00",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T14:16:06.455367Z",
     "start_time": "2025-12-03T14:16:06.439194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import colorsys\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from colorthief import ColorThief\n",
    "\n",
    "class VisionModule:\n",
    "    \n",
    "    def __init__(self, feature_extraction_model, classifier, configuration):\n",
    "        self.feature_extraction_model = feature_extraction_model\n",
    "        self.classifier = classifier\n",
    "        self.configuration = configuration\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def crop(self, img, save_dir=None):\n",
    "\n",
    "        crops = {}\n",
    "        crop_cfg = self.configuration[\"crop_positions\"]\n",
    "\n",
    "        for key, cfg in crop_cfg.items():\n",
    "            \n",
    "            # --- 1. Crop the region ---\n",
    "            x1, y1, x2, y2 = cfg[\"x1\"], cfg[\"y1\"], cfg[\"x2\"], cfg[\"y2\"]\n",
    "            crop_img = img[y1:y2, x1:x2].copy()\n",
    "            \n",
    "            # --- 2. Apply circular mask ONLY if mask_radius_factor exists ---\n",
    "            if cfg.get(\"apply_circular_mask\", False) is True:\n",
    "                radius = cfg.get(\"radius\", None)\n",
    "                center = cfg.get(\"center\", None)\n",
    "                center = (center[0] - x1, center[1] - y1)\n",
    "                crop_img = self._apply_circular_mask(\n",
    "                    crop_img,\n",
    "                    radius,\n",
    "                    center\n",
    "                )\n",
    "\n",
    "            # --- 3. Optionally save cropped image ---\n",
    "            if save_dir is not None:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                save_path = os.path.join(save_dir, f\"{key}.png\")\n",
    "                cv2.imwrite(save_path, cv2.cvtColor(crop_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            # --- 4. Add to output dictionary ---\n",
    "            crops[key] = crop_img\n",
    "\n",
    "        return crops\n",
    "\n",
    "\n",
    "    def _apply_circular_mask(self, img, radius, center):\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        if center is not None:\n",
    "            center = (int(center[0]), int(center[1]))\n",
    "        else:\n",
    "            center = (int(w // 2), int(h // 2))\n",
    "            \n",
    "        if radius is not None:\n",
    "            radius = int(radius)\n",
    "        else:\n",
    "            radius = int(min(h, w) * 0.9 / 2)\n",
    "    \n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    \n",
    "        # make white circle on a center\n",
    "        cv2.circle(mask, center, radius, (255, 255, 255), -1)\n",
    "    \n",
    "        mask = mask.astype(float) / 255.0\n",
    "        \n",
    "        fill_color = [0.485*255, 0.456*255, 0.406*255]\n",
    "        fill_color = np.array(fill_color, dtype=np.float32)\n",
    "        \n",
    "        fill_img = np.ones_like(img, dtype=np.float32) * fill_color\n",
    "    \n",
    "        result = img.astype(float) * mask[..., None] + fill_img * (1 - mask[..., None])\n",
    "    \n",
    "        return result.astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    def process_crops(self, crops):\n",
    "        processed = {}\n",
    "        \n",
    "        for key, img in crops.items():\n",
    "            # Skip text panel for CNN\n",
    "            if key == \"text_panel\":\n",
    "                processed[key] = img\n",
    "                continue\n",
    "            \n",
    "            processed[key] = self._preprocess_for_cnn(img)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    \n",
    "    def _preprocess_for_cnn(self, cropped_img):\n",
    "        # resize\n",
    "        size = self.configuration[\"resize\"]\n",
    "        img = cv2.resize(cropped_img, size)\n",
    "    \n",
    "        # convert to float32 0–1\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "        # ImageNet norm (change if you use custom model)\n",
    "        mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "        std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    \n",
    "        img = (img - mean) / std\n",
    "    \n",
    "        # HWC → CHW\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "    \n",
    "        return img\n",
    "    \n",
    "    \n",
    "    def extract_deep_features(self, tensor):\n",
    "\n",
    "        t = torch.from_numpy(tensor).unsqueeze(0).float()  \n",
    "        # shape (1,3,224,224) for resnet 50\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            deep_features = self.feature_extraction_model(t).cpu().numpy().flatten()\n",
    "        torch.save()\n",
    "        return deep_features\n",
    "    \n",
    "\n",
    "    def handcrafted_features(self, cropped_img_pth):\n",
    "            \n",
    "        colors_platte = ColorThief(cropped_img_pth)\n",
    "        dominant_colors = colors_platte.get_palette(color_count=2)\n",
    "        print(dominant_colors)\n",
    "        \n",
    "        ignored_colors = [\n",
    "            (0, 0, 0),\n",
    "            (255, 255, 255),\n",
    "            (int(0.485*255), int(0.456*255), int(0.406*255))  # fill mask base color\n",
    "        ]\n",
    "        tol = 15\n",
    "        cleaned_colors = []\n",
    "        \n",
    "        for R, G, B in dominant_colors:\n",
    "            bad = False\n",
    "            for ir, ig, ib in ignored_colors:\n",
    "                if (abs(R - ir) <= tol and\n",
    "                    abs(G - ig) <= tol and\n",
    "                    abs(B - ib) <= tol):\n",
    "                    bad = True\n",
    "                    break\n",
    "        \n",
    "            if not bad:\n",
    "                cleaned_colors.append((R, G, B))\n",
    "        \n",
    "        if len(cleaned_colors) == 0:\n",
    "            cleaned_colors = [(0,0,0)]\n",
    "        \n",
    "        R, G, B = cleaned_colors[0]\n",
    "        r, g, b = R/255.0, G/255.0, B/255.0\n",
    "        \n",
    "        h, s, v = colorsys.rgb_to_hsv(r, g, b)\n",
    "        \n",
    "        dom_h = h\n",
    "        dom_s = s\n",
    "        dom_v = v\n",
    "        \n",
    "        # ---- FIXED BGR → RGB for cv2 ----\n",
    "        cropped_img = cv2.imread(cropped_img_pth)\n",
    "        cropped_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        img = cropped_img.astype(np.float32) / 255.0\n",
    "        hsv = cv2.cvtColor(cropped_img, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "    \n",
    "        # Create mask of \"valid\" pixels matrix/grid\n",
    "        valid_mask = np.ones((img.shape[0], img.shape[1]), dtype=bool)\n",
    "    \n",
    "        for ir, ig, ib in ignored_colors:\n",
    "            if (ir, ig, ib) == ignored_colors[2]:\n",
    "                # This is the FILL MASK → use tolerance\n",
    "                invalid = (\n",
    "                    (np.abs(cropped_img[:,:,0] - ir) < tol) &\n",
    "                    (np.abs(cropped_img[:,:,1] - ig) < tol) &\n",
    "                    (np.abs(cropped_img[:,:,2] - ib) < tol)\n",
    "                )\n",
    "            else:\n",
    "                # Black & white → exact match\n",
    "                invalid = (\n",
    "                    (cropped_img[:,:,0] == ir) &\n",
    "                    (cropped_img[:,:,1] == ig) &\n",
    "                    (cropped_img[:,:,2] == ib)\n",
    "                )\n",
    "        \n",
    "            valid_mask[invalid] = False\n",
    "    \n",
    "        # Valid pixels only\n",
    "        valid_hsv = hsv[valid_mask]\n",
    "    \n",
    "        # Intensity Features (based on V channel)\n",
    "        V = valid_hsv[:,2] / 255.0\n",
    "    \n",
    "        avg_intensity = float(V.mean())\n",
    "        std_intensity = float(V.std())\n",
    "    \n",
    "        # Center vs Periphery intensities, the center sharpness\n",
    "        h_im, w_im = cropped_img.shape[:2]\n",
    "        cy, cx = h_im//2, w_im//2\n",
    "        R = min(cx, cy)\n",
    "    \n",
    "        r_center = int(R * 0.3)\n",
    "        r_mid = int(R * 0.60)\n",
    "    \n",
    "        Y, X = np.ogrid[:h_im, :w_im]\n",
    "        dist = np.sqrt((X - cx)**2 + (Y - cy)**2)\n",
    "    \n",
    "        center_mask = dist < r_center\n",
    "        periphery_mask = (dist > r_mid) & (dist < R)\n",
    "    \n",
    "        center_vals = img[center_mask][:, :].mean() if center_mask.any() else 0.0\n",
    "        periphery_vals = img[periphery_mask][:, :].mean() if periphery_mask.any() else 1e-6\n",
    "        \n",
    "        center_intensity = float(center_vals)\n",
    "        periphery_intensity = float(periphery_vals)\n",
    "        center_periphery_ratio = float(center_intensity / periphery_intensity)\n",
    "    \n",
    "        # Quadrant splits and asymmetry ratios\n",
    "        top = img[:h_im//2,:,:].mean()\n",
    "        bottom = img[h_im//2:,:,:].mean()\n",
    "        left = img[:, :w_im//2, :].mean()\n",
    "        right = img[:, w_im//2:, :].mean()\n",
    "    \n",
    "        inferior_superior_ratio = float(bottom / (top + 1e-6))\n",
    "        left_right_ratio = float(left / (right + 1e-6))\n",
    "    \n",
    "        diag1 = img[:h_im//2, :w_im//2, :].mean() - img[h_im//2:, w_im//2:, :].mean()\n",
    "        diag2 = img[:h_im//2, w_im//2:, :].mean() - img[h_im//2:, :w_im//2, :].mean()\n",
    "    \n",
    "        diag1_difference = float(diag1)\n",
    "        diag2_difference = float(diag2)\n",
    "    \n",
    "        radial_symmetry = float(\n",
    "            abs(top - bottom) +\n",
    "            abs(left - right) +\n",
    "            abs(diag1) +\n",
    "            abs(diag2)\n",
    "        )\n",
    "    \n",
    "        return {\n",
    "            \"dom_h\": dom_h,\n",
    "            \"dom_s\": dom_s,\n",
    "            \"dom_v\": dom_v,\n",
    "    \n",
    "            \"avg_intensity\": avg_intensity,\n",
    "            \"std_intensity\": std_intensity,\n",
    "    \n",
    "            \"center_intensity\": center_intensity,\n",
    "            \"periphery_intensity\": periphery_intensity,\n",
    "            \"center_periphery_ratio\": center_periphery_ratio,\n",
    "    \n",
    "            \"inferior_superior_ratio\": inferior_superior_ratio,\n",
    "            \"left_right_ratio\": left_right_ratio,\n",
    "    \n",
    "            \"diag1_difference\": diag1_difference,\n",
    "            \"diag2_difference\": diag2_difference,\n",
    "    \n",
    "            \"radial_symmetry\": radial_symmetry\n",
    "        }\n",
    "    \n",
    "        \n",
    "    \n",
    "    def run_vision_preprocessing(self, img_pth):\n",
    "       \n",
    "        img = cv2.imread(img_pth)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # STEP 1 — Crop\n",
    "        crops = self.crop(img)\n",
    "    \n",
    "        # STEP 2 — Preprocess for CNN\n",
    "        processed = self.process_crops(crops)\n",
    "        \n",
    "        # start thread here for extract_deep_features and handcrafted_features\n",
    "        \n",
    "        vision_output = {}\n",
    "    \n",
    "        # STEP 3 — Quadrants\n",
    "        for key in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
    "            pass\n",
    "         \n",
    "        vision_output[\"text_panel\"] = crops[\"text_panel\"]\n",
    "    \n",
    "        return vision_output\n",
    "    \n",
    "    def run_vision_training(self):\n",
    "        pass\n",
    "\n",
    "\n"
   ],
   "id": "199bcba3ab306a9d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T14:16:07.281672Z",
     "start_time": "2025-12-03T14:16:07.213477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "obj = VisionModule(feature_extraction_model=\"p\",classifier=\"g\", configuration=configuration)\n",
    "\n",
    "img = cv2.imread(r\"dataset/cropping_sample/Q2.png\")\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "h = obj.handcrafted_features(cropped_img_pth=r\"dataset/cropping_sample/Q2.png\")\n",
    "\n",
    "print(h)\n"
   ],
   "id": "f464e43d167dc7ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(119, 112, 95), (118, 239, 16), (233, 246, 51)]\n",
      "{'dom_h': 0.2571001494768311, 'dom_s': 0.9330543933054394, 'dom_v': 0.9372549019607843, 'avg_intensity': 0.9078391790390015, 'std_intensity': 0.17134372889995575, 'center_intensity': 0.569532573223114, 'periphery_intensity': 0.467157781124115, 'center_periphery_ratio': 1.2191439300286426, 'inferior_superior_ratio': 0.9308066368103027, 'left_right_ratio': 1.0133957862854004, 'diag1_difference': 0.04128372669219971, 'diag2_difference': 0.02835288643836975, 'radial_symmetry': 0.11092042922973633}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T11:14:45.651733Z",
     "start_time": "2025-12-03T11:14:45.647199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class TextModule:\n",
    "#     def __init__(self, ocr_engine, text_classifier, embed_compressor):\n",
    "#         self.ocr = ocr_engine\n",
    "#         self.text_classifier = text_classifier\n",
    "#         self.embed_compressor = embed_compressor\n",
    "# \n",
    "#     def ocr_extract(self, crop): ...\n",
    "#     def clean_text(self, raw): ...\n",
    "#     def extract_numeric(self, text): ...\n",
    "#     def classify_text(self, text): ...\n",
    "#     def build_output_dict(self, ...): ...\n",
    "# \n",
    "#     def run(self, OCR_crop):\n",
    "#         raw = self.ocr_extract(OCR_crop)\n",
    "#         cleaned = self.clean_text(raw)\n",
    "#         numeric = self.extract_numeric(cleaned)\n",
    "#         text_pred, text_probs, embed = self.classify_text(cleaned)\n",
    "# \n",
    "#         # build Text_Output_Dict\n",
    "#         return Text_Output_Dict\n"
   ],
   "id": "24ec2fe2097b2143",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cdbc774d9921b0bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# class FusionModule:\n",
    "#     def __init__(self, xgb_model):\n",
    "#         self.model = xgb_model\n",
    "# \n",
    "#     def build_fusion_vector(self, image_dict, text_dict): ...\n",
    "#     def run(self, Image_Output_Dict, Text_Output_Dict):\n",
    "#         x = self.build_fusion_vector(Image_Output_Dict, Text_Output_Dict)\n",
    "#         pred = self.model.predict(x)\n",
    "#         conf = self.model.predict_proba(x)\n",
    "#         return Fusion_Output_Dict\n"
   ],
   "id": "91e3cc60629fccf8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# class ReportModule:\n",
    "#     def __init__(self, t5_model, tokenizer):\n",
    "#         self.model = t5_model\n",
    "#         self.tokenizer = tokenizer\n",
    "# \n",
    "#     def build_long_text(self, image_dict, text_dict, fusion_dict): ...\n",
    "#     def summarize(self, text): ...\n",
    "# \n",
    "#     def run(self, Image_Output_Dict, Text_Output_Dict, Fusion_Output_Dict):\n",
    "#         full = self.build_long_text(...)\n",
    "#         summary = self.summarize(full)\n",
    "#         return summary\n"
   ],
   "id": "9eab50940608515b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# def run_pipeline(image_path):\n",
    "#     # 1 Vision\n",
    "#     Image_Output_Dict, OCR_crop = VisionModule.run(image_path)\n",
    "# \n",
    "#     # 2 Text\n",
    "#     Text_Output_Dict = TextModule.run(OCR_crop)\n",
    "# \n",
    "#     # 3 Fusion\n",
    "#     Fusion_Output_Dict = FusionModule.run(Image_Output_Dict, Text_Output_Dict)\n",
    "# \n",
    "#     # 4 Report\n",
    "#     final_report = ReportModule.run(Image_Output_Dict, Text_Output_Dict, Fusion_Output_Dict)\n",
    "# \n",
    "#     return {\n",
    "#         \"vision\": Image_Output_Dict,\n",
    "#         \"text\": Text_Output_Dict,\n",
    "#         \"fusion\": Fusion_Output_Dict,\n",
    "#         \"report\": final_report\n",
    "#     }\n"
   ],
   "id": "e50156560e60733"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
