{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# class ImageClassifier(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim=64, num_classes=5):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "#         \n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "# torch.save(image_classifier.state_dict(), \"image_classifier.pt\")\n"
   ],
   "id": "ea59977f722e076"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class VisionModule:\n",
    "#     def __init__(self, resnet_model, image_classifier):\n",
    "#         self.resnet = resnet_model\n",
    "#         self.image_classifier = image_classifier\n",
    "# \n",
    "#     def load_image(self, path): ...\n",
    "#     def extract_deep_features(self, q): ...\n",
    "#     def extract_handcrafted(self, q): ...\n",
    "#     def build_output_dict(self, ...): ...\n",
    "# \n",
    "#     def run(self, image_path):\n",
    "#         img = self.load_image(image_path)\n",
    "#         Q1, Q2, Q3, Q4, OCR_crop = self.crop_quadrants(img)\n",
    "# \n",
    "#         # run all quadrant processing\n",
    "#         # build Image_Output_Dict\n",
    "#         return Image_Output_Dict, OCR_crop\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T19:10:37.527918Z",
     "start_time": "2025-12-01T19:10:37.521883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "configuration = {\n",
    "\n",
    "    \"resize\": (224, 224),\n",
    "\n",
    "    \"crop_positions\": {\n",
    "        \n",
    "        \"Q1\": {\"x1\": 772, \"y1\": 91, \"x2\": 1194, \"y2\": 451, \"center\": (967,275), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q2\": {\"x1\": 336, \"y1\": 90, \"x2\": 759, \"y2\": 453, \"center\": (592, 275), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q3\": {\"x1\": 336, \"y1\": 467, \"x2\": 758, \"y2\": 830, \"center\": (592,652), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"Q4\": {\"x1\": 771, \"y1\": 467, \"x2\": 1194, \"y2\": 830, \"center\": (967,652), \"radius\":143, \"apply_circular_mask\":True},\n",
    "        \n",
    "        \"text_panel\": {\"x1\": 11, \"y1\": 203, \"x2\": 324, \"y2\": 828}\n",
    "    }\n",
    "}\n"
   ],
   "id": "d1a19ba5c7ccbc00",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:04:45.402401Z",
     "start_time": "2025-12-01T18:04:45.395334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class VisionModule:\n",
    "    def __init__(self, feature_extraction_model, classifier, configuration):\n",
    "        self.feature_extraction_model = feature_extraction_model\n",
    "        self.classifier = classifier\n",
    "        self.configuration = configuration\n",
    "        \n",
    "    def crop(self, img, save_dir=None):\n",
    "\n",
    "        crops = {}\n",
    "        crop_cfg = self.configuration[\"crop_positions\"]\n",
    "\n",
    "        for key, cfg in crop_cfg.items():\n",
    "            \n",
    "            # --- 1. Crop the region ---\n",
    "            x1, y1, x2, y2 = cfg[\"x1\"], cfg[\"y1\"], cfg[\"x2\"], cfg[\"y2\"]\n",
    "            crop_img = img[y1:y2, x1:x2].copy()\n",
    "            \n",
    "            # --- 2. Apply circular mask ONLY if mask_radius_factor exists ---\n",
    "            if cfg.get(\"apply_circular_mask\", False) is True:\n",
    "                radius = cfg.get(\"radius\", None)\n",
    "                center = cfg.get(\"center\", None)\n",
    "                center = (center[0] - x1, center[1] - y1)\n",
    "                crop_img = self._apply_circular_mask(\n",
    "                    crop_img,\n",
    "                    radius,\n",
    "                    center\n",
    "                )\n",
    "\n",
    "            # --- 3. Optionally save cropped image ---\n",
    "            if save_dir is not None:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                save_path = os.path.join(save_dir, f\"{key}.png\")\n",
    "                cv2.imwrite(save_path, cv2.cvtColor(crop_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            # --- 4. Add to output dictionary ---\n",
    "            crops[key] = crop_img\n",
    "\n",
    "        return crops\n",
    "\n",
    "\n",
    "    def _apply_circular_mask(self, img, radius, center):\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        if center is not None:\n",
    "            center = (int(center[0]), int(center[1]))\n",
    "        else:\n",
    "            center = (int(w // 2), int(h // 2))\n",
    "            \n",
    "        if radius is not None:\n",
    "            radius = int(radius)\n",
    "        else:\n",
    "            radius = int(min(h, w) * 0.9 / 2)\n",
    "    \n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    \n",
    "        # make white circle on a center\n",
    "        cv2.circle(mask, center, radius, (255, 255, 255), -1)\n",
    "    \n",
    "        mask = mask.astype(float) / 255.0\n",
    "        \n",
    "        fill_color = [0.485*255, 0.456*255, 0.406*255]\n",
    "        fill_color = np.array(fill_color, dtype=np.float32)\n",
    "        \n",
    "        fill_img = np.ones_like(img, dtype=np.float32) * fill_color\n",
    "    \n",
    "        result = img.astype(float) * mask[..., None] + fill_img * (1 - mask[..., None])\n",
    "    \n",
    "        return result.astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    def process_crops(self, crops):\n",
    "        processed = {}\n",
    "        \n",
    "        for key, img in crops.items():\n",
    "            # Skip text panel for CNN\n",
    "            if key == \"text_panel\":\n",
    "                processed[key] = img\n",
    "                continue\n",
    "            \n",
    "            processed[key] = self._preprocess_for_cnn(img)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    \n",
    "    def _preprocess_for_cnn(self, cropped_img):\n",
    "        # resize\n",
    "        size = self.configuration[\"resize\"]\n",
    "        img = cv2.resize(cropped_img, size)\n",
    "    \n",
    "        # convert to float32 0–1\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "        # ImageNet norm (change if you use custom model)\n",
    "        mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "        std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    \n",
    "        img = (img - mean) / std\n",
    "    \n",
    "        # HWC → CHW\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "    \n",
    "        return img\n",
    "    \n",
    "    \n",
    "    def extract_deep_features(self, tensor):\n",
    "\n",
    "        t = torch.from_numpy(tensor).unsqueeze(0).float()  \n",
    "        # shape (1,3,224,224) for resnet 50\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            deep_features = self.feature_extraction_model(t).cpu().numpy().flatten()\n",
    "    \n",
    "        return deep_features\n",
    "    \n",
    "\n",
    "    def handcrafted_features(self, crop_img):\n",
    "        hsv = cv2.cvtColor(crop_img, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "        mean_rgb = crop_img.mean(axis=(0,1))\n",
    "        mean_hsv = hsv.mean(axis=(0,1))\n",
    "    \n",
    "        return {\n",
    "            \"dom_r\": float(mean_rgb[0]),\n",
    "            \"dom_g\": float(mean_rgb[1]),\n",
    "            \"dom_b\": float(mean_rgb[2]),\n",
    "            \"mean_h\": float(mean_hsv[0]),\n",
    "            \"mean_s\": float(mean_hsv[1]),\n",
    "            \"mean_v\": float(mean_hsv[2]),\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def run_vision(self, img):\n",
    "        # Step 1 — Crop\n",
    "        crops = self.crop(img)\n",
    "    \n",
    "        # Step 2 — Normalize crops for CNN\n",
    "        processed = self.process_crops(crops)\n",
    "    \n",
    "        vision_output = {}\n",
    "    \n",
    "        # Step 3 — For each quadrant\n",
    "        for key in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
    "            \n",
    "            # deep features\n",
    "            deep_feat = self.extract_deep_features(processed[key])\n",
    "    \n",
    "            # handcrafted features\n",
    "            hand_feat = self.handcrafted_features(crops[key])\n",
    "    \n",
    "            # combine features\n",
    "            full_feat = np.concatenate(\n",
    "                [deep_feat, np.array(list(hand_feat.values()))]\n",
    "            )\n",
    "    \n",
    "            # Step 4 — Prediction\n",
    "            pred = self.classifier.predict(full_feat.reshape(1, -1))[0]\n",
    "    \n",
    "            # Store results\n",
    "            vision_output[key] = {\n",
    "                \"deep_features\": deep_feat,\n",
    "                \"handcrafted\": hand_feat,\n",
    "                \"prediction\": pred\n",
    "            }\n",
    "    \n",
    "        # Step 5 — Add text panel image\n",
    "        vision_output[\"text_panel\"] = crops[\"text_panel\"]\n",
    "    \n",
    "        return vision_output\n",
    "\n",
    "\n"
   ],
   "id": "199bcba3ab306a9d",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:04:45.996040Z",
     "start_time": "2025-12-01T18:04:45.909636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "obj = VisionModule(feature_extraction_model=\"p\",classifier=\"g\", configuration=configuration)\n",
    "\n",
    "img = cv2.imread(r\"dataset/normal/207.jpg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "obj.crop(img=img, save_dir=r\"dataset/cropping_sample\")"
   ],
   "id": "f464e43d167dc7ae",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': array([[[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]]], shape=(360, 422, 3), dtype=uint8),\n",
       " 'Q2': array([[[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]]], shape=(363, 423, 3), dtype=uint8),\n",
       " 'Q3': array([[[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]]], shape=(363, 422, 3), dtype=uint8),\n",
       " 'Q4': array([[[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]],\n",
       " \n",
       "        [[123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         ...,\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103],\n",
       "         [123, 116, 103]]], shape=(363, 423, 3), dtype=uint8),\n",
       " 'text_panel': array([[[244, 244, 244],\n",
       "         [240, 240, 240],\n",
       "         [239, 239, 239],\n",
       "         ...,\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         [239, 239, 239]],\n",
       " \n",
       "        [[243, 243, 243],\n",
       "         [241, 241, 241],\n",
       "         [240, 240, 240],\n",
       "         ...,\n",
       "         [244, 244, 244],\n",
       "         [244, 244, 244],\n",
       "         [245, 245, 245]],\n",
       " \n",
       "        [[244, 244, 244],\n",
       "         [242, 242, 242],\n",
       "         [242, 242, 242],\n",
       "         ...,\n",
       "         [240, 240, 240],\n",
       "         [235, 235, 235],\n",
       "         [249, 249, 249]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[244, 244, 244],\n",
       "         [242, 242, 242],\n",
       "         [240, 240, 240],\n",
       "         ...,\n",
       "         [247, 247, 247],\n",
       "         [242, 242, 242],\n",
       "         [243, 243, 243]],\n",
       " \n",
       "        [[230, 230, 230],\n",
       "         [248, 248, 248],\n",
       "         [229, 229, 229],\n",
       "         ...,\n",
       "         [250, 250, 250],\n",
       "         [248, 248, 248],\n",
       "         [243, 243, 243]],\n",
       " \n",
       "        [[249, 249, 249],\n",
       "         [245, 245, 245],\n",
       "         [243, 243, 243],\n",
       "         ...,\n",
       "         [232, 232, 232],\n",
       "         [239, 239, 239],\n",
       "         [248, 248, 248]]], shape=(625, 313, 3), dtype=uint8)}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# class TextModule:\n",
    "#     def __init__(self, ocr_engine, text_classifier, embed_compressor):\n",
    "#         self.ocr = ocr_engine\n",
    "#         self.text_classifier = text_classifier\n",
    "#         self.embed_compressor = embed_compressor\n",
    "# \n",
    "#     def ocr_extract(self, crop): ...\n",
    "#     def clean_text(self, raw): ...\n",
    "#     def extract_numeric(self, text): ...\n",
    "#     def classify_text(self, text): ...\n",
    "#     def build_output_dict(self, ...): ...\n",
    "# \n",
    "#     def run(self, OCR_crop):\n",
    "#         raw = self.ocr_extract(OCR_crop)\n",
    "#         cleaned = self.clean_text(raw)\n",
    "#         numeric = self.extract_numeric(cleaned)\n",
    "#         text_pred, text_probs, embed = self.classify_text(cleaned)\n",
    "# \n",
    "#         # build Text_Output_Dict\n",
    "#         return Text_Output_Dict\n"
   ],
   "id": "24ec2fe2097b2143"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cdbc774d9921b0bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# class FusionModule:\n",
    "#     def __init__(self, xgb_model):\n",
    "#         self.model = xgb_model\n",
    "# \n",
    "#     def build_fusion_vector(self, image_dict, text_dict): ...\n",
    "#     def run(self, Image_Output_Dict, Text_Output_Dict):\n",
    "#         x = self.build_fusion_vector(Image_Output_Dict, Text_Output_Dict)\n",
    "#         pred = self.model.predict(x)\n",
    "#         conf = self.model.predict_proba(x)\n",
    "#         return Fusion_Output_Dict\n"
   ],
   "id": "91e3cc60629fccf8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# class ReportModule:\n",
    "#     def __init__(self, t5_model, tokenizer):\n",
    "#         self.model = t5_model\n",
    "#         self.tokenizer = tokenizer\n",
    "# \n",
    "#     def build_long_text(self, image_dict, text_dict, fusion_dict): ...\n",
    "#     def summarize(self, text): ...\n",
    "# \n",
    "#     def run(self, Image_Output_Dict, Text_Output_Dict, Fusion_Output_Dict):\n",
    "#         full = self.build_long_text(...)\n",
    "#         summary = self.summarize(full)\n",
    "#         return summary\n"
   ],
   "id": "9eab50940608515b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# def run_pipeline(image_path):\n",
    "#     # 1 Vision\n",
    "#     Image_Output_Dict, OCR_crop = VisionModule.run(image_path)\n",
    "# \n",
    "#     # 2 Text\n",
    "#     Text_Output_Dict = TextModule.run(OCR_crop)\n",
    "# \n",
    "#     # 3 Fusion\n",
    "#     Fusion_Output_Dict = FusionModule.run(Image_Output_Dict, Text_Output_Dict)\n",
    "# \n",
    "#     # 4 Report\n",
    "#     final_report = ReportModule.run(Image_Output_Dict, Text_Output_Dict, Fusion_Output_Dict)\n",
    "# \n",
    "#     return {\n",
    "#         \"vision\": Image_Output_Dict,\n",
    "#         \"text\": Text_Output_Dict,\n",
    "#         \"fusion\": Fusion_Output_Dict,\n",
    "#         \"report\": final_report\n",
    "#     }\n"
   ],
   "id": "e50156560e60733"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
