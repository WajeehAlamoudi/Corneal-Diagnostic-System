{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:52:09.268996Z",
     "start_time": "2025-12-04T13:52:07.771999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "feature_extractor = nn.Sequential(*list(backbone.children())[:-1])\n",
    "feature_extractor = feature_extractor.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "dcde4263e8fbbef8",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:52:10.107330Z",
     "start_time": "2025-12-04T13:52:10.101329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from classes import VisionModule\n",
    "from config import configuration\n",
    "\n",
    "vision_model_obj = VisionModule( feature_extraction_model=feature_extractor, configuration=configuration )"
   ],
   "id": "4ad37c86f6e22e5f",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T18:44:36.552787Z",
     "start_time": "2025-12-04T18:44:35.574481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from classes import TextModule\n",
    "\n",
    "text_model_obj = TextModule()"
   ],
   "id": "24ec2fe2097b2143",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T20:31:08.875404Z",
     "start_time": "2025-12-04T20:31:05.259544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "\n",
    "def test_architecture_pipeline(img_path):\n",
    "\n",
    "    print(\"\\n==================== TEST PIPELINE ====================\")\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 1. Preprocess → get crops folder\n",
    "    # ---------------------------------------------------\n",
    "    crops_path = vision_model_obj.run_image_preprocessing(img_path)\n",
    "\n",
    "    if crops_path is None or not os.path.exists(crops_path):\n",
    "        print(\"[ERROR] run_image_preprocessing() did not return a valid folder.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 2. Prepare save dirs (TEMP STORAGE for test)\n",
    "    # ---------------------------------------------------\n",
    "    base = os.path.dirname(os.path.dirname(img_path))\n",
    "\n",
    "    deep_dir = os.path.join(base, \"test_deep_features\")\n",
    "    hand_dir = os.path.join(base, \"test_hand_features\")\n",
    "    trans_dir = os.path.join(base, \"test_transformer_features\")\n",
    "\n",
    "    os.makedirs(deep_dir, exist_ok=True)\n",
    "    os.makedirs(hand_dir, exist_ok=True)\n",
    "    os.makedirs(trans_dir, exist_ok=True)\n",
    "\n",
    "    img_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 3. Extract features for each quadrant\n",
    "    # ---------------------------------------------------\n",
    "    quadrant_list = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
    "    quadrant_vectors = []\n",
    "    handcrafted_all_quadrants = {}\n",
    "\n",
    "    for q in quadrant_list:\n",
    "\n",
    "        crop_path = os.path.join(crops_path, f\"{img_id}_{q}.png\")\n",
    "\n",
    "        if not os.path.exists(crop_path):\n",
    "            print(f\"[WARN] Missing crop: {crop_path}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # LOAD CROP\n",
    "        crop = cv2.imread(crop_path)\n",
    "        crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # DEEP FEATURES\n",
    "        # ---------------------------------------------------\n",
    "        cnn_ready = vision_model_obj.preprocess_for_cnn(crop)\n",
    "        deep_vec = vision_model_obj.extract_deep_features(\n",
    "            tensor=cnn_ready,\n",
    "            save_dir=deep_dir,\n",
    "            img_name=img_id,\n",
    "            quadrant=q\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # HANDCRAFT FEATURES\n",
    "        # ---------------------------------------------------\n",
    "        hand_vec = vision_model_obj.handcrafted_features(\n",
    "            cropped_img_pth=crop_path,\n",
    "            save_dir=hand_dir,\n",
    "            img_name=img_id,\n",
    "            quadrant=q\n",
    "        )\n",
    "        handcrafted_all_quadrants[q] = hand_vec\n",
    "\n",
    "        hand_vec = np.array(list(hand_vec.values()), dtype=np.float32)\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # TRANSFORMER FEATURES\n",
    "        # ---------------------------------------------------\n",
    "        trans_vec = text_model_obj.extract_transformer_features(\n",
    "            img_path=crop_path,\n",
    "            save_dir=trans_dir,\n",
    "            img_name=img_id,\n",
    "            quadrant=q\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # CONCATENATE 3 MODALITIES FOR THIS QUADRANT\n",
    "        # ---------------------------------------------------\n",
    "        fused_vec = np.concatenate([deep_vec, hand_vec, trans_vec], axis=0)\n",
    "        quadrant_vectors.append(fused_vec)\n",
    "\n",
    "    if len(quadrant_vectors) == 0:\n",
    "        print(\"[ERROR] No valid quadrant vectors extracted.\")\n",
    "        return None\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 4. FINAL IMAGE FEATURE = AVERAGE OF QUADRANTS\n",
    "    # ---------------------------------------------------\n",
    "    final_feature_vec = np.mean(quadrant_vectors, axis=0)\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 5. LOAD TRAINED XGBOOST MODEL\n",
    "    # ---------------------------------------------------\n",
    "    model_path = r\"saved_models/kc_classifier.pkl\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"[ERROR] XGBoost model file not found:\", model_path)\n",
    "        return None\n",
    "\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 6. PREDICTION\n",
    "    # ---------------------------------------------------\n",
    "    pred_prob = model.predict_proba(final_feature_vec.reshape(1, -1))[0]\n",
    "    pred_class = model.predict(final_feature_vec.reshape(1, -1))[0]\n",
    "\n",
    "    label_map = {0: \"normal\", 1: \"Keratoconus\"}\n",
    "    pred_label = label_map[int(pred_class)]\n",
    "\n",
    "    print(\"\\n==================== RESULT ====================\")\n",
    "    print(f\"Prediction: {pred_label}\")\n",
    "    print(f\"Probability: normal={pred_prob[0]:.4f}, KC={pred_prob[1]:.4f}\")\n",
    "    print(\"=================================================\\n\")\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 7. OUTPUTS FOR LLM REPORT\n",
    "    # ---------------------------------------------------\n",
    "    final_prediction = {\n",
    "        \"image_id\": img_id,\n",
    "        \"predicted_label\": pred_label,\n",
    "        \"probabilities\": {\n",
    "            \"normal\": float(pred_prob[0]),\n",
    "            \"keratoconus\": float(pred_prob[1])\n",
    "        },\n",
    "        \"handcrafted_summary\": handcrafted_all_quadrants\n",
    "    }\n",
    "    \n",
    "    plain_prompt = f\"\"\"\n",
    "        Image ID: {img_id}\n",
    "        \n",
    "        Prediction: {pred_label}\n",
    "        Probability Normal: {final_prediction['probabilities']['normal']:.4f}\n",
    "        Probability Keratoconus: {final_prediction['probabilities']['keratoconus']:.4f}\n",
    "        \n",
    "        Handcrafted features per quadrant:\n",
    "        {handcrafted_all_quadrants}\n",
    "        \n",
    "        Please summarize this result clinically.\n",
    "        \"\"\"\n",
    "    \n",
    "    return final_prediction, plain_prompt\n",
    "    "
   ],
   "id": "d218f0a077b249bc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 58707953-f3d2-4bbb-913c-2d0299f7a6bb)')' thrown while requesting HEAD https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/preprocessor_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T20:57:10.854672Z",
     "start_time": "2025-12-04T20:57:10.834808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown, display\n",
    "import ipywidgets as widgets\n",
    "from classes import GEMINIAGENT\n",
    "\n",
    "def run_with_chat(img_path):\n",
    "    # 1. Run your vision + XGBoost pipeline\n",
    "    final_prediction, prompt_text = test_architecture_pipeline(img_path)\n",
    "\n",
    "    # 2. Create Gemini agent\n",
    "    agent = GEMINIAGENT(\n",
    "        api_key=\"AIzaSyC1r4egWoskczgSYmXkxCQMhJ6szqOw6XM\",\n",
    "        system_message=\"\"\"\n",
    "        You are a medical assistant and OCULUS - PENTACAM 4 Maps analysis.\n",
    "        \"\"\",\n",
    "        model=\"gemini-2.5-flash\"\n",
    "    )\n",
    "\n",
    "    # 3. Auto clinical report\n",
    "    report = agent.ask(prompt_text)\n",
    "    display(Markdown(\"## Clinical Report\\n\\n\" + report))\n",
    "\n",
    "    # -------- CHAT MODE --------\n",
    "    print(\"\\n===== CHAT MODE =====\")\n",
    "    \n",
    "    # Create output area for chat history\n",
    "    chat_output = widgets.Output()\n",
    "    \n",
    "    input_box = widgets.Text(\n",
    "        placeholder=\"Type your question...\",\n",
    "        description=\"You:\",\n",
    "        layout=widgets.Layout(width=\"500px\")\n",
    "    )\n",
    "\n",
    "    send_button = widgets.Button(\n",
    "        description=\"Send\",\n",
    "        button_style=\"success\"\n",
    "    )\n",
    "    \n",
    "    # Container for input controls\n",
    "    input_container = widgets.HBox([input_box, send_button])\n",
    "\n",
    "    def send_message(_):\n",
    "        user_input = input_box.value.strip()\n",
    "        if not user_input:\n",
    "            return\n",
    "\n",
    "        input_box.value = \"\"  # Clear box\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            with chat_output:\n",
    "                print(\"Chat ended.\")\n",
    "            return\n",
    "\n",
    "        # Display user message\n",
    "        with chat_output:\n",
    "            display(Markdown(f\"**You:** {user_input}\"))\n",
    "        \n",
    "        # Get and display assistant reply\n",
    "        try:\n",
    "            reply = agent.ask(user_input)\n",
    "            with chat_output:\n",
    "                display(Markdown(f\"**Assistant:** {reply}\\n\\n---\\n\"))\n",
    "        except Exception as e:\n",
    "            with chat_output:\n",
    "                display(Markdown(f\"**Error:** {str(e)}\\n\\n---\\n\"))\n",
    "\n",
    "    # Allow Enter key to send message\n",
    "    def handle_submit(sender):\n",
    "        send_message(None)\n",
    "    \n",
    "    input_box.on_submit(handle_submit)\n",
    "    send_button.on_click(send_message)\n",
    "    \n",
    "    # Display the interface\n",
    "    display(input_container)\n",
    "    display(chat_output)"
   ],
   "id": "83ee79112e03d6d",
   "outputs": [],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T20:57:29.718191Z",
     "start_time": "2025-12-04T20:57:11.058035Z"
    }
   },
   "cell_type": "code",
   "source": "run_with_chat(\"dataset/test/images/2.jpg\")",
   "id": "8e6bfdaeedbb6e88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== TEST PIPELINE ====================\n",
      "\n",
      "==================== RESULT ====================\n",
      "Prediction: normal\n",
      "Probability: normal=0.6984, KC=0.3016\n",
      "=================================================\n",
      "\n",
      "Initializing GEMINIAGENT chat for model gemini-2.5-flash.\n",
      "GEMINIAGENT initialized successfully for model gemini-2.5-flash.\n",
      "GEMINIAGENT.ask() completed successfully with reply length 2206 chars.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Clinical Report\n\n**Clinical Summary of Oculus Pentacam 4 Maps Analysis (Image ID: 2)**\n\nThe automated analysis predicts a **normal cornea** with a probability of 69.84%. This prediction is well-supported by the detailed evaluation of handcrafted features across all four quadrants.\n\n**Key Findings:**\n\n1.  **Overall Symmetry:** The various symmetry metrics consistently indicate a healthy, symmetrical corneal shape.\n    *   **Inferior-Superior Ratios** are all very close to 1 (ranging from 0.999 to 1.014), indicating excellent vertical symmetry without significant inferior steepening, which is a hallmark of keratoconus.\n    *   **Left-Right Ratios** are also close to 1 (ranging from 0.980 to 0.995), suggesting good horizontal symmetry.\n    *   **Radial Symmetry** values are low across all quadrants (0.006 to 0.032), confirming a high degree of radial regularity and an absence of localized, asymmetric bulging.\n    *   **Diagonal Differences (diag1_difference, diag2_difference)** are very small, further supporting overall corneal symmetry.\n\n2.  **Corneal Shape and Smoothness:**\n    *   The **average intensity** is consistently high across all quadrants (0.867 to 0.894), and the **standard deviation of intensity** is low (0.147 to 0.180). This suggests a generally uniform and smooth corneal surface without significant localized irregularities or abrupt changes in elevation/curvature.\n    *   **Dominant Value (dom_v)** is similar across all quadrants (0.46 to 0.50), implying a consistent distribution of the underlying topographical feature represented by intensity.\n\n3.  **Center-Periphery Relationship:** While there are some variations in the center-periphery ratio per quadrant (e.g., Q3 at 0.981 indicating a slightly less intense center compared to the periphery, while Q1 and Q4 are >1), these variations do not present a consistent pattern indicative of a pathological cone.\n\n**Conclusion:**\n\nBased on the quantitative analysis of the provided handcrafted features, there are **no clinical signs suggestive of keratoconus or other significant corneal ectasia**. The cornea demonstrates excellent symmetry and regularity, strongly supporting the automated prediction of a normal corneal topography."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CHAT MODE =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wajee\\AppData\\Local\\Temp\\ipykernel_94924\\3278937598.py:70: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  input_box.on_submit(handle_submit)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(Text(value='', description='You:', layout=Layout(width='500px'), placeholder='Type your questio…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a75aabd92ee479ca7aff9233bfc34bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07ed462017094e68b50444bcae259340"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 165
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a5bc449dbcff0def"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
